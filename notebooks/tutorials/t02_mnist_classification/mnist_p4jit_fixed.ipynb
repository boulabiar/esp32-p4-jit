{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST INT8 Classification on ESP32-P4 using P4-JIT\n",
    "\n",
    "## ✅ Complete Fixed Pipeline\n",
    "\n",
    "**Key Fixes:**\n",
    "- ✅ **Correct bias scaling**: `bias × 2^E_in` (not `2^(E_in+E_w-E_out)`)\n",
    "- ✅ **Input quantization** from the start\n",
    "- ✅ **Proper preprocessing** match between training and deployment\n",
    "- ✅ **Individual scale exponents** passed to C for clarity\n",
    "- ✅ **Smart Args** for clean NumPy interface\n",
    "- ✅ **Verification steps** at each stage\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment ready\n",
      "✓ PyTorch: 2.8.0+cu126\n",
      "✓ Device: CUDA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "# Setup directories\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "SOURCE_DIR = NOTEBOOK_DIR / \"source\"\n",
    "WEIGHTS_DIR = NOTEBOOK_DIR / \"weights\"\n",
    "RESULTS_DIR = NOTEBOOK_DIR / \"results\"\n",
    "\n",
    "for d in [SOURCE_DIR, WEIGHTS_DIR, RESULTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add P4-JIT\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent.parent\n",
    "sys.path.append(str(PROJECT_ROOT / \"host\"))\n",
    "\n",
    "from p4jit import P4JIT, MALLOC_CAP_SPIRAM, MALLOC_CAP_8BIT\n",
    "import p4jit\n",
    "\n",
    "# Seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Environment ready\")\n",
    "print(f\"✓ PyTorch: {torch.__version__}\")\n",
    "print(f\"✓ Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation\n",
    "\n",
    "**Important:** MNIST images are normalized with `mean=0.1307, std=0.3081` during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "print(f\"✓ Train: {len(train_dataset):,} | Test: {len(test_dataset):,}\")\n",
    "\n",
    "# Visualize (denormalized for display)\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n",
    "for i in range(20):\n",
    "    img, label = train_dataset[i]\n",
    "    # Denormalize for visualization\n",
    "    img_display = img.squeeze() * 0.3081 + 0.1307\n",
    "    ax = axes[i // 10, i % 10]\n",
    "    ax.imshow(img_display, cmap='gray')\n",
    "    ax.set_title(f'{label}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('MNIST Dataset Samples', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'dataset.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quantization Modules\n",
    "\n",
    "**Power-of-2 scales** with Straight-Through Estimator for gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Quantization modules defined\n"
     ]
    }
   ],
   "source": [
    "class PowerOfTwoQuantize(torch.autograd.Function):\n",
    "    \"\"\"INT8 quantization with Straight-Through Estimator\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, scale_exp):\n",
    "        scale = 2.0 ** scale_exp\n",
    "        x_q = torch.clamp(torch.round(x * scale), -128, 127)\n",
    "        x_dq = x_q / scale\n",
    "        return x_dq\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, None\n",
    "\n",
    "\n",
    "class FakeQuantizeINT8(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.register_buffer('scale_exp', torch.tensor(0))\n",
    "        self.enabled = False\n",
    "        \n",
    "    def set_scale_exp(self, exp):\n",
    "        self.scale_exp = torch.tensor(exp)\n",
    "        \n",
    "    def enable(self):\n",
    "        self.enabled = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not self.enabled:\n",
    "            return x\n",
    "        return PowerOfTwoQuantize.apply(x, self.scale_exp)\n",
    "    \n",
    "    def get_scale_info(self):\n",
    "        exp = int(self.scale_exp.item())\n",
    "        scale = 2.0 ** (-exp)\n",
    "        return exp, scale\n",
    "\n",
    "\n",
    "class QuantizedConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, **kwargs)\n",
    "        self.weight_quant = FakeQuantizeINT8()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        w_q = self.weight_quant(self.conv.weight)\n",
    "        return F.conv2d(x, w_q, self.conv.bias, \n",
    "                       self.conv.stride, self.conv.padding, \n",
    "                       self.conv.dilation, self.conv.groups)\n",
    "\n",
    "\n",
    "class QuantizedLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.weight_quant = FakeQuantizeINT8()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        w_q = self.weight_quant(self.linear.weight)\n",
    "        return F.linear(x, w_q, self.linear.bias)\n",
    "\n",
    "\n",
    "print(\"✓ Quantization modules defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture\n",
    "\n",
    "**With input quantization from the start!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model: 206,922 parameters\n",
      "✓ Input quantization: ENABLED\n",
      "QuantizedMNISTNet(\n",
      "  (input_quant): FakeQuantizeINT8()\n",
      "  (conv1): QuantizedConv2d(\n",
      "    (conv): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (weight_quant): FakeQuantizeINT8()\n",
      "  )\n",
      "  (act1_quant): FakeQuantizeINT8()\n",
      "  (conv2): QuantizedConv2d(\n",
      "    (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (weight_quant): FakeQuantizeINT8()\n",
      "  )\n",
      "  (act2_quant): FakeQuantizeINT8()\n",
      "  (fc1): QuantizedLinear(\n",
      "    (linear): Linear(in_features=1568, out_features=128, bias=True)\n",
      "    (weight_quant): FakeQuantizeINT8()\n",
      "  )\n",
      "  (act3_quant): FakeQuantizeINT8()\n",
      "  (fc2): QuantizedLinear(\n",
      "    (linear): Linear(in_features=128, out_features=10, bias=True)\n",
      "    (weight_quant): FakeQuantizeINT8()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class QuantizedMNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input quantization\n",
    "        self.input_quant = FakeQuantizeINT8()\n",
    "        \n",
    "        # Conv layers\n",
    "        self.conv1 = QuantizedConv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.act1_quant = FakeQuantizeINT8()\n",
    "        \n",
    "        self.conv2 = QuantizedConv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.act2_quant = FakeQuantizeINT8()\n",
    "        \n",
    "        # FC layers\n",
    "        self.fc1 = QuantizedLinear(32 * 7 * 7, 128)\n",
    "        self.act3_quant = FakeQuantizeINT8()\n",
    "        \n",
    "        self.fc2 = QuantizedLinear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_quant(x)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.act1_quant(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.act2_quant(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.act3_quant(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = QuantizedMNISTNet().to(device)\n",
    "\n",
    "print(f\"✓ Model: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"✓ Input quantization: ENABLED\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training: Warmup → Calibration → QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 1: WARMUP (FP32)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d26807292e4eccb9243442208893a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 | Train: 93.80% | Test: 98.23%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8ddd443415456c9fbb5808a9a4d968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 | Train: 98.16% | Test: 98.48%\n",
      "✓ Warmup complete: 98.48%\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for data, target in tqdm(loader, desc='Training', leave=False):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        correct += output.argmax(dim=1).eq(target).sum().item()\n",
    "    return total_loss / len(loader), 100. * correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += criterion(output, target).item()\n",
    "            correct += output.argmax(dim=1).eq(target).sum().item()\n",
    "    return total_loss / len(loader), 100. * correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "WARMUP_EPOCHS = 2\n",
    "QAT_EPOCHS = 2\n",
    "history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
    "\n",
    "# Phase 1: Warmup\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1: WARMUP (FP32)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for epoch in range(1, WARMUP_EPOCHS + 1):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{WARMUP_EPOCHS} | Train: {train_acc:.2f}% | Test: {test_acc:.2f}%\")\n",
    "\n",
    "print(f\"✓ Warmup complete: {history['test_acc'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 2: CALIBRATION\n",
      "================================================================================\n",
      "\n",
      "Weight calibration:\n",
      "  conv1                → Exp:  +7 (scale: 2^7)\n",
      "  conv2                → Exp:  +8 (scale: 2^8)\n",
      "  fc1                  → Exp:  +9 (scale: 2^9)\n",
      "  fc2                  → Exp:  +9 (scale: 2^9)\n",
      "\n",
      "Activation calibration:\n",
      "  input_quant          → Exp:  +5 (scale: 2^5)\n",
      "  act1_quant           → Exp:  +4 (scale: 2^4)\n",
      "  act2_quant           → Exp:  +4 (scale: 2^4)\n",
      "  act3_quant           → Exp:  +2 (scale: 2^2)\n",
      "\n",
      "✓ Calibration complete\n"
     ]
    }
   ],
   "source": [
    "def calculate_scale_exponent(tensor):\n",
    "    max_val = tensor.abs().max().item()\n",
    "    if max_val == 0:\n",
    "        return 0\n",
    "    import math\n",
    "    return math.floor(math.log2(127.0 / max_val))\n",
    "\n",
    "\n",
    "def calibrate_activations(model, loader, device, num_batches=10):\n",
    "    model.eval()\n",
    "    activation_stats = {}\n",
    "    \n",
    "    def get_hook(name):\n",
    "        def hook(module, input, output):\n",
    "            if name not in activation_stats:\n",
    "                activation_stats[name] = []\n",
    "            activation_stats[name].append(output.abs().max().item())\n",
    "        return hook\n",
    "    \n",
    "    hooks = []\n",
    "    \n",
    "    # Hook for input\n",
    "    def input_hook(module, input):\n",
    "        if 'input_quant' not in activation_stats:\n",
    "            activation_stats['input_quant'] = []\n",
    "        activation_stats['input_quant'].append(input[0].abs().max().item())\n",
    "    \n",
    "    hooks.append(model.register_forward_pre_hook(input_hook))\n",
    "    \n",
    "    # Hooks for activations\n",
    "    for name, module in model.named_modules():\n",
    "        if 'act' in name and isinstance(module, FakeQuantizeINT8):\n",
    "            parent_name = '.'.join(name.split('.')[:-1])\n",
    "            if parent_name:\n",
    "                parent = dict(model.named_modules())[parent_name]\n",
    "            else:\n",
    "                parent = module\n",
    "            hooks.append(parent.register_forward_hook(get_hook(name)))\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(loader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            model(data.to(device))\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    exponents = {}\n",
    "    for name, values in activation_stats.items():\n",
    "        max_val = max(values)\n",
    "        import math\n",
    "        exp = math.floor(math.log2(127.0 / max_val)) if max_val > 0 else 0\n",
    "        exponents[name] = exp\n",
    "    \n",
    "    return exponents\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2: CALIBRATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Weights\n",
    "print(\"\\nWeight calibration:\")\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, (QuantizedConv2d, QuantizedLinear)):\n",
    "        weight = module.conv.weight if isinstance(module, QuantizedConv2d) else module.linear.weight\n",
    "        exp = calculate_scale_exponent(weight.data)\n",
    "        module.weight_quant.set_scale_exp(exp)\n",
    "        print(f\"  {name:20s} → Exp: {exp:+3d} (scale: 2^{exp})\")\n",
    "\n",
    "# Activations\n",
    "print(\"\\nActivation calibration:\")\n",
    "act_exponents = calibrate_activations(model, train_loader, device, num_batches=10)\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, FakeQuantizeINT8) and name in act_exponents:\n",
    "        exp = act_exponents[name]\n",
    "        module.set_scale_exp(exp)\n",
    "        print(f\"  {name:20s} → Exp: {exp:+3d} (scale: 2^{exp})\")\n",
    "\n",
    "print(\"\\n✓ Calibration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QAT Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 3: QUANTIZATION-AWARE TRAINING\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62338b86eedb4c7a9e9bdc97b8950be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4 | Train: 98.85% | Test: 98.67%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e692bec6eb514b13a1bcf0bf0ad873cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4 | Train: 99.11% | Test: 98.79%\n",
      "\n",
      "================================================================================\n",
      "✓ Final Test Accuracy: 98.79%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Enable quantization\n",
    "for module in model.modules():\n",
    "    if isinstance(module, FakeQuantizeINT8):\n",
    "        module.enable()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 3: QUANTIZATION-AWARE TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "for epoch in range(1, QAT_EPOCHS + 1):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    total_epoch = WARMUP_EPOCHS + epoch\n",
    "    print(f\"Epoch {total_epoch}/{WARMUP_EPOCHS + QAT_EPOCHS} | Train: {train_acc:.2f}% | Test: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"✓ Final Test Accuracy: {history['test_acc'][-1]:.2f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract Quantized Weights\n",
    "\n",
    "**Store all scale exponents for C code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "WEIGHT EXTRACTION\n",
      "================================================================================\n",
      "\n",
      "Input: E_in = 5\n",
      "Conv1: E_w= +7, E_act= +4, W_shape=(16, 1, 3, 3)\n",
      "Conv2: E_w= +8, E_act= +4, W_shape=(32, 16, 3, 3)\n",
      "FC1:   E_w= +9, E_act= +2, W_shape=(128, 1568)\n",
      "FC2:   E_w= +9, W_shape=(10, 128)\n",
      "\n",
      "✓ Total: 206,922 bytes (202.07 KB)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def quantize_to_int8(tensor, scale_exp):\n",
    "    scale = 2.0 ** scale_exp\n",
    "    quantized = torch.clamp(torch.round(tensor * scale), -128, 127).to(torch.int8)\n",
    "    return quantized.cpu().numpy()\n",
    "\n",
    "\n",
    "model.eval()\n",
    "quantized_weights = {}\n",
    "scale_exponents = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WEIGHT EXTRACTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Input\n",
    "input_exp, _ = model.input_quant.get_scale_info()\n",
    "scale_exponents['input'] = input_exp\n",
    "print(f\"\\nInput: E_in = {input_exp}\")\n",
    "\n",
    "# Conv1\n",
    "w_exp, _ = model.conv1.weight_quant.get_scale_info()\n",
    "act_exp, _ = model.act1_quant.get_scale_info()\n",
    "w = quantize_to_int8(model.conv1.conv.weight.data, w_exp)\n",
    "b = quantize_to_int8(model.conv1.conv.bias.data, w_exp)\n",
    "quantized_weights['conv1'] = {'weight': w, 'bias': b}\n",
    "scale_exponents['conv1'] = {'w': w_exp, 'act': act_exp}\n",
    "print(f\"Conv1: E_w={w_exp:+3d}, E_act={act_exp:+3d}, W_shape={w.shape}\")\n",
    "\n",
    "# Conv2\n",
    "w_exp, _ = model.conv2.weight_quant.get_scale_info()\n",
    "act_exp, _ = model.act2_quant.get_scale_info()\n",
    "w = quantize_to_int8(model.conv2.conv.weight.data, w_exp)\n",
    "b = quantize_to_int8(model.conv2.conv.bias.data, w_exp)\n",
    "quantized_weights['conv2'] = {'weight': w, 'bias': b}\n",
    "scale_exponents['conv2'] = {'w': w_exp, 'act': act_exp}\n",
    "print(f\"Conv2: E_w={w_exp:+3d}, E_act={act_exp:+3d}, W_shape={w.shape}\")\n",
    "\n",
    "# FC1\n",
    "w_exp, _ = model.fc1.weight_quant.get_scale_info()\n",
    "act_exp, _ = model.act3_quant.get_scale_info()\n",
    "w = quantize_to_int8(model.fc1.linear.weight.data, w_exp)\n",
    "b = quantize_to_int8(model.fc1.linear.bias.data, w_exp)\n",
    "quantized_weights['fc1'] = {'weight': w, 'bias': b}\n",
    "scale_exponents['fc1'] = {'w': w_exp, 'act': act_exp}\n",
    "print(f\"FC1:   E_w={w_exp:+3d}, E_act={act_exp:+3d}, W_shape={w.shape}\")\n",
    "\n",
    "# FC2\n",
    "w_exp, _ = model.fc2.weight_quant.get_scale_info()\n",
    "w = quantize_to_int8(model.fc2.linear.weight.data, w_exp)\n",
    "b = quantize_to_int8(model.fc2.linear.bias.data, w_exp)\n",
    "quantized_weights['fc2'] = {'weight': w, 'bias': b}\n",
    "scale_exponents['fc2'] = {'w': w_exp}\n",
    "print(f\"FC2:   E_w={w_exp:+3d}, W_shape={w.shape}\")\n",
    "\n",
    "total_params = sum(w['weight'].size + w['bias'].size for w in quantized_weights.values())\n",
    "print(f\"\\n✓ Total: {total_params:,} bytes ({total_params/1024:.2f} KB)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. C Implementation with FIXED Bias Scaling\n",
    "\n",
    "### The Correct Math:\n",
    "\n",
    "```\n",
    "Accumulator = Σ(input_q × weight_q)\n",
    "            = Σ((input × 2^E_in) × (weight × 2^E_w))\n",
    "            = (Σ input × weight) × 2^(E_in + E_w)\n",
    "\n",
    "Bias at scale 2^E_w needs to be brought to scale 2^(E_in + E_w):\n",
    "  bias_scaled = bias_q × 2^E_in\n",
    "\n",
    "Then scale to output:\n",
    "  output_q = (acc + bias_scaled) >> (E_in + E_w - E_out)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ C code saved: c:\\Users\\orani\\bilel\\git_projects\\robert_manzke\\project1\\trys\\costume_p4code_binary\\P4-JIT\\notebooks\\tutorials\\t02_mnist_classification\\source\\mnist_inference.c\n",
      "✓ Size: 8119 bytes\n",
      "✓ FIX 1: ReLU applied BEFORE quantization\n",
      "✓ FIX 2: FC2 outputs INT32 logits\n",
      "✓ NEW: Cycle counter timing added\n"
     ]
    }
   ],
   "source": [
    "c_code = \"\"\"#include <stdint.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "// Read RISC-V cycle counter\n",
    "static inline uint32_t rdcycle(void) {\n",
    "    uint32_t cycles;\n",
    "    asm volatile (\"rdcycle %0\" : \"=r\"(cycles));\n",
    "    return cycles;\n",
    "}\n",
    "\n",
    "static inline int32_t relu_int32(int32_t x) { return (x > 0) ? x : 0; }\n",
    "static inline int8_t clip_int8(int32_t x) {\n",
    "    return (x > 127) ? 127 : ((x < -128) ? -128 : (int8_t)x);\n",
    "}\n",
    "\n",
    "// Conv2d with ReLU BEFORE quantization\n",
    "void conv2d_int8(\n",
    "    const int8_t* input, int in_h, int in_w, int in_c,\n",
    "    const int8_t* weight, const int8_t* bias,\n",
    "    int8_t* output, int out_c,\n",
    "    int exp_in, int exp_w, int exp_out\n",
    ") {\n",
    "    const int K = 3, P = 1, S = 1;\n",
    "    int out_h = (in_h + 2*P - K) / S + 1;\n",
    "    int out_w = (in_w + 2*P - K) / S + 1;\n",
    "    \n",
    "    int acc_shift = exp_in + exp_w - exp_out;\n",
    "    \n",
    "    for (int oc = 0; oc < out_c; oc++) {\n",
    "        for (int oh = 0; oh < out_h; oh++) {\n",
    "            for (int ow = 0; ow < out_w; ow++) {\n",
    "                int32_t acc = 0;\n",
    "                \n",
    "                // MAC\n",
    "                for (int ic = 0; ic < in_c; ic++) {\n",
    "                    for (int kh = 0; kh < K; kh++) {\n",
    "                        for (int kw = 0; kw < K; kw++) {\n",
    "                            int ih = oh * S - P + kh;\n",
    "                            int iw = ow * S - P + kw;\n",
    "                            \n",
    "                            if (ih >= 0 && ih < in_h && iw >= 0 && iw < in_w) {\n",
    "                                acc += (int32_t)input[(ic * in_h + ih) * in_w + iw] *\n",
    "                                       (int32_t)weight[((oc * in_c + ic) * K + kh) * K + kw];\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                // Add bias (scaled by E_in)\n",
    "                acc += (int32_t)bias[oc] << exp_in;\n",
    "                \n",
    "                // ReLU BEFORE quantization\n",
    "                acc = relu_int32(acc);\n",
    "                \n",
    "                // Scale to output\n",
    "                acc = acc >> acc_shift;\n",
    "                \n",
    "                output[(oc * out_h + oh) * out_w + ow] = clip_int8(acc);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "void maxpool2d_int8(const int8_t* input, int8_t* output, int h, int w, int c) {\n",
    "    int out_h = h / 2, out_w = w / 2;\n",
    "    for (int ch = 0; ch < c; ch++) {\n",
    "        for (int oh = 0; oh < out_h; oh++) {\n",
    "            for (int ow = 0; ow < out_w; ow++) {\n",
    "                int8_t max_val = -128;\n",
    "                for (int kh = 0; kh < 2; kh++) {\n",
    "                    for (int kw = 0; kw < 2; kw++) {\n",
    "                        int8_t v = input[(ch * h + oh*2 + kh) * w + ow*2 + kw];\n",
    "                        if (v > max_val) max_val = v;\n",
    "                    }\n",
    "                }\n",
    "                output[(ch * out_h + oh) * out_w + ow] = max_val;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// FC layer with ReLU BEFORE quantization\n",
    "void fc_int8(\n",
    "    const int8_t* input, int in_size,\n",
    "    const int8_t* weight, const int8_t* bias,\n",
    "    int8_t* output, int out_size,\n",
    "    int exp_in, int exp_w, int exp_out\n",
    ") {\n",
    "    int acc_shift = exp_in + exp_w - exp_out;\n",
    "    \n",
    "    for (int i = 0; i < out_size; i++) {\n",
    "        int32_t acc = 0;\n",
    "        for (int j = 0; j < in_size; j++) {\n",
    "            acc += (int32_t)input[j] * (int32_t)weight[i * in_size + j];\n",
    "        }\n",
    "        \n",
    "        // Add bias (scaled by E_in)\n",
    "        acc += (int32_t)bias[i] << exp_in;\n",
    "        \n",
    "        // ReLU BEFORE quantization\n",
    "        acc = relu_int32(acc);\n",
    "        \n",
    "        // Scale to output\n",
    "        acc = acc >> acc_shift;\n",
    "        \n",
    "        output[i] = clip_int8(acc);\n",
    "    }\n",
    "}\n",
    "\n",
    "// FC layer WITHOUT quantization (for final layer)\n",
    "void fc_int32(\n",
    "    const int8_t* input, int in_size,\n",
    "    const int8_t* weight, const int8_t* bias,\n",
    "    int32_t* output, int out_size,\n",
    "    int exp_in, int exp_w\n",
    ") {\n",
    "    for (int i = 0; i < out_size; i++) {\n",
    "        int32_t acc = 0;\n",
    "        for (int j = 0; j < in_size; j++) {\n",
    "            acc += (int32_t)input[j] * (int32_t)weight[i * in_size + j];\n",
    "        }\n",
    "        \n",
    "        // Add bias (scaled by E_in)\n",
    "        acc += (int32_t)bias[i] << exp_in;\n",
    "        \n",
    "        // NO ReLU, NO quantization - raw logits\n",
    "        output[i] = acc;\n",
    "    }\n",
    "}\n",
    "\n",
    "void fc_int32_p4simd(\n",
    "    const int8_t* input, int in_size,\n",
    "    const int8_t* weight, const int8_t* bias,\n",
    "    int32_t* output, int out_size,\n",
    "    int exp_in, int exp_w\n",
    ") {\n",
    "    // Current pointer to weights (increments monotonically)\n",
    "    const int8_t* w_ptr = weight;\n",
    "    \n",
    "    // Calculate loop iterations (16 bytes per iteration)\n",
    "    int loop_count = in_size >> 4; \n",
    "\n",
    "    for (int i = 0; i < out_size; i++) {\n",
    "        // CRITICAL FIX: ESP32-P4 PIE instructions are restricted to the \n",
    "        // a0-a5 (x10-x15) register range. We must force variables into these registers.\n",
    "        \n",
    "        register const int8_t* in_ptr asm(\"a0\") = input;\n",
    "        register const int8_t* cur_w_ptr asm(\"a1\") = w_ptr;\n",
    "        register int32_t acc_val asm(\"a2\");\n",
    "        register int cnt asm(\"a3\") = loop_count;\n",
    "        register int shift_reg asm(\"a4\") = 0;\n",
    "\n",
    "        asm volatile (\n",
    "            \"esp.zero.xacc \\\\n\\\\t\"                   // Clear accumulator\n",
    "            \"esp.lp.setup 0, %[cnt], 1f \\\\n\\\\t\"      // Setup HW loop 0\n",
    "            \"0: \\\\n\\\\t\"                              // Loop start label\n",
    "            \"esp.vld.128.ip q0, %[in], 16 \\\\n\\\\t\"    // Load 16 inputs (uses a0)\n",
    "            \"esp.vld.128.ip q1, %[w], 16 \\\\n\\\\t\"     // Load 16 weights (uses a1)\n",
    "            \"esp.vmulas.s8.xacc q0, q1 \\\\n\\\\t\"       // Multiply & Accumulate\n",
    "            \"1: \\\\n\\\\t\"                              // Loop end label\n",
    "            \"esp.srs.s.xacc %[res], %[shft] \\\\n\\\\t\"  // Extract result (uses a2, a4)\n",
    "\n",
    "            // Output Operands\n",
    "            : [res] \"=r\" (acc_val),       // Mapped to a2\n",
    "              [in]  \"+r\" (in_ptr),        // Mapped to a0\n",
    "              [w]   \"+r\" (cur_w_ptr)      // Mapped to a1\n",
    "            \n",
    "            // Input Operands\n",
    "            : [cnt] \"r\" (cnt),            // Mapped to a3\n",
    "              [shft] \"r\" (shift_reg)      // Mapped to a4\n",
    "            \n",
    "            // Clobbers\n",
    "            : \"memory\"\n",
    "        );\n",
    "\n",
    "        w_ptr = cur_w_ptr; \n",
    "        output[i] = acc_val + ((int32_t)bias[i] << exp_in);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Inference function WITH TIMING\n",
    "int32_t mnist_inference(\n",
    "    int8_t* input,\n",
    "    int8_t* w_conv1, int8_t* b_conv1,\n",
    "    int8_t* w_conv2, int8_t* b_conv2,\n",
    "    int8_t* w_fc1, int8_t* b_fc1,\n",
    "    int8_t* w_fc2, int8_t* b_fc2,\n",
    "    int32_t e_in,\n",
    "    int32_t e_conv1_w, int32_t e_conv1_act,\n",
    "    int32_t e_conv2_w, int32_t e_conv2_act,\n",
    "    int32_t e_fc1_w, int32_t e_fc1_act,\n",
    "    int32_t e_fc2_w,\n",
    "    int8_t* scratch,\n",
    "    uint32_t* timing  \n",
    ") {\n",
    "    // Start timing\n",
    "    uint32_t start_cycles = rdcycle();\n",
    "    \n",
    "    printf(\"[JIT] Inference start\\\\n\");\n",
    "    \n",
    "    int8_t* conv1_out = scratch;\n",
    "    int8_t* pool1_out = conv1_out + 16*28*28;\n",
    "    int8_t* conv2_out = pool1_out + 16*14*14;\n",
    "    int8_t* pool2_out = conv2_out + 32*14*14;\n",
    "    int8_t* fc1_out = pool2_out + 32*7*7;\n",
    "    \n",
    "    // Allocate INT32 buffer for final logits\n",
    "    int32_t* fc2_out = (int32_t*)(fc1_out + 128);\n",
    "    \n",
    "    // Conv1 + ReLU + MaxPool\n",
    "    conv2d_int8(input, 28, 28, 1, w_conv1, b_conv1, conv1_out, 16, \n",
    "                e_in, e_conv1_w, e_conv1_act);\n",
    "    maxpool2d_int8(conv1_out, pool1_out, 28, 28, 16);\n",
    "    \n",
    "    // Conv2 + ReLU + MaxPool\n",
    "    conv2d_int8(pool1_out, 14, 14, 16, w_conv2, b_conv2, conv2_out, 32,\n",
    "                e_conv1_act, e_conv2_w, e_conv2_act);\n",
    "    maxpool2d_int8(conv2_out, pool2_out, 14, 14, 32);\n",
    "    \n",
    "    // FC1 + ReLU\n",
    "    fc_int8(pool2_out, 1568, w_fc1, b_fc1, fc1_out, 128,\n",
    "            e_conv2_act, e_fc1_w, e_fc1_act);\n",
    "    \n",
    "    // FC2 → INT32 logits\n",
    "    fc_int32_p4simd(fc1_out, 128, w_fc2, b_fc2, fc2_out, 10,\n",
    "             e_fc1_act, e_fc2_w);\n",
    "    \n",
    "    // End timing\n",
    "    uint32_t end_cycles = rdcycle();\n",
    "    uint32_t elapsed = end_cycles - start_cycles;\n",
    "    \n",
    "    // Store timing in output array\n",
    "    timing[0] = elapsed;\n",
    "    \n",
    "    // Argmax\n",
    "    int32_t max_val = fc2_out[0];\n",
    "    int32_t max_idx = 0;\n",
    "    for (int i = 1; i < 10; i++) {\n",
    "        if (fc2_out[i] > max_val) {\n",
    "            max_val = fc2_out[i];\n",
    "            max_idx = i;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    printf(\"[JIT] Predicted: %d (logit: %d) | Cycles: %u\\\\n\", max_idx, max_val, elapsed);\n",
    "    return max_idx;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "c_path = SOURCE_DIR / \"mnist_inference.c\"\n",
    "with open(c_path, 'w') as f:\n",
    "    f.write(c_code)\n",
    "\n",
    "print(f\"✓ C code saved: {c_path}\")\n",
    "print(f\"✓ Size: {len(c_code)} bytes\")\n",
    "print(\"✓ FIX 1: ReLU applied BEFORE quantization\")\n",
    "print(\"✓ FIX 2: FC2 outputs INT32 logits\")\n",
    "print(\"✓ NEW: Cycle counter timing added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prepare Test Images\n",
    "\n",
    "**Match preprocessing from training!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_raw = {}\n",
    "test_images_int8 = {}\n",
    "\n",
    "input_exp = scale_exponents['input']\n",
    "input_scale = 2.0 ** input_exp\n",
    "\n",
    "for digit in range(10):\n",
    "    for img, label in test_dataset:\n",
    "        if label == digit:\n",
    "            test_images_raw[digit] = img\n",
    "            \n",
    "            # Quantize NORMALIZED image (matching training)\n",
    "            img_np = img.squeeze().numpy()  # Already normalized!\n",
    "            img_int8 = np.clip(np.round(img_np * input_scale), -128, 127).astype(np.int8)\n",
    "            test_images_int8[digit] = img_int8.flatten()\n",
    "            break\n",
    "\n",
    "print(f\"✓ Test images prepared\")\n",
    "print(f\"  Input scale: 2^{input_exp} = {input_scale}\")\n",
    "print(f\"  Range: [{test_images_int8[0].min()}, {test_images_int8[0].max()}]\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for digit in range(10):\n",
    "    ax = axes[digit // 5, digit % 5]\n",
    "    # Denormalize for display\n",
    "    img_display = test_images_raw[digit].squeeze() * 0.3081 + 0.1307\n",
    "    ax.imshow(img_display, cmap='gray')\n",
    "    ax.set_title(f'Digit {digit}', fontweight='bold')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Test Images', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'test_images.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Deploy to ESP32-P4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ESP32-P4 DEPLOYMENT\n",
      "================================================================================\n",
      "15:38:29 [p4jit.p4jit] \u001b[94mINFO\u001b[0m: Initializing P4JIT System...\n",
      "15:38:29 [p4jit.runtime.jit_session] \u001b[94mINFO\u001b[0m: Auto-detecting JIT device...\n",
      "15:38:29 [p4jit.runtime.device_manager] \u001b[94mINFO\u001b[0m: Connecting to COM3 at 115200 baud...\n",
      "15:38:29 [p4jit.runtime.device_manager] \u001b[94mINFO\u001b[0m: Connecting to COM6 at 115200 baud...\n",
      "15:38:29 [p4jit.runtime.device_manager] \u001b[94mINFO\u001b[0m: Connected.\n",
      "15:38:29 [p4jit.runtime.jit_session] \u001b[94mINFO\u001b[0m: Found JIT Device at COM6\n",
      "15:38:29 [p4jit.p4jit] \u001b[94mINFO\u001b[0m: P4JIT Initialized.\n",
      "15:38:29 [p4jit.p4jit] \u001b[94mINFO\u001b[0m: [Heap Params]\n",
      "15:38:29 [p4jit.p4jit] \u001b[94mINFO\u001b[0m:   free_spiram    :   31388992 bytes (30653.31 KB)\n",
      "15:38:29 [p4jit.p4jit] \u001b[94mINFO\u001b[0m:   total_spiram   :   33554432 bytes (32768.00 KB)\n",
      "15:38:29 [p4jit.p4jit] \u001b[94mINFO\u001b[0m:   free_internal  :     384063 bytes (375.06 KB)\n",
      "15:38:29 [p4jit.p4jit] \u001b[94mINFO\u001b[0m:   total_internal :     464119 bytes (453.24 KB)\n",
      "\n",
      "Compiling kernel...\n",
      "15:38:29 [p4jit.p4jit] \u001b[94mINFO\u001b[0m: Loading 'mnist_inference' from 'mnist_inference.c'...\n",
      "15:38:29 [p4jit.toolchain.wrapper_builder] \u001b[94mINFO\u001b[0m: Generating wrapper for 'mnist_inference'\n",
      "15:38:29 [p4jit.toolchain.wrapper_builder] \u001b[94mINFO\u001b[0m: Building wrapper binary...\n",
      "15:38:29 [p4jit.toolchain.builder] \u001b[94mINFO\u001b[0m: Discovered 2 source file(s) in c:\\Users\\orani\\bilel\\git_projects\\robert_manzke\\project1\\trys\\costume_p4code_binary\\P4-JIT\\notebooks\\tutorials\\t02_mnist_classification\\source\n",
      "15:38:29 [p4jit.toolchain.builder] \u001b[94mINFO\u001b[0m: Build validation passed\n",
      "15:38:29 [p4jit.toolchain.wrapper_builder] \u001b[94mINFO\u001b[0m: Wrapper build complete. Metadata saved to c:\\Users\\orani\\bilel\\git_projects\\robert_manzke\\project1\\trys\\costume_p4code_binary\\P4-JIT\\notebooks\\tutorials\\t02_mnist_classification\\build\\signature.json\n",
      "15:38:29 [p4jit.p4jit] \u001b[94mINFO\u001b[0m:   Code Allocated: 0x48210AE0 (4428 bytes)\n",
      "15:38:29 [p4jit.p4jit] \u001b[94mINFO\u001b[0m:   Args Allocated: 0x48211C30 (128 bytes)\n",
      "15:38:29 [p4jit.toolchain.wrapper_builder] \u001b[94mINFO\u001b[0m: Generating wrapper for 'mnist_inference'\n",
      "15:38:29 [p4jit.toolchain.wrapper_builder] \u001b[94mINFO\u001b[0m: Building wrapper binary...\n",
      "15:38:29 [p4jit.toolchain.builder] \u001b[94mINFO\u001b[0m: Discovered 2 source file(s) in c:\\Users\\orani\\bilel\\git_projects\\robert_manzke\\project1\\trys\\costume_p4code_binary\\P4-JIT\\notebooks\\tutorials\\t02_mnist_classification\\source\n",
      "15:38:30 [p4jit.toolchain.builder] \u001b[94mINFO\u001b[0m: Build validation passed\n",
      "15:38:30 [p4jit.toolchain.wrapper_builder] \u001b[94mINFO\u001b[0m: Wrapper build complete. Metadata saved to c:\\Users\\orani\\bilel\\git_projects\\robert_manzke\\project1\\trys\\costume_p4code_binary\\P4-JIT\\notebooks\\tutorials\\t02_mnist_classification\\build\\signature.json\n",
      "15:38:30 [p4jit.p4jit] \u001b[94mINFO\u001b[0m: Function loaded successfully.\n",
      "\n",
      "✓ Loaded at: 0x48210AE0\n",
      "✓ Binary: 4372 bytes (4.27 KB)\n",
      "15:38:30 [p4jit.toolchain.binary_object] \u001b[94mINFO\u001b[0m: Disassembly saved to c:\\Users\\orani\\bilel\\git_projects\\robert_manzke\\project1\\trys\\costume_p4code_binary\\P4-JIT\\notebooks\\tutorials\\t02_mnist_classification\\source\\disassembly.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ESP32-P4 DEPLOYMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "jit = P4JIT()\n",
    "stats_initial = jit.get_heap_stats(print_s=True)\n",
    "\n",
    "print(\"\\nCompiling kernel...\")\n",
    "p4jit.set_log_level('INFO')\n",
    "\n",
    "func = jit.load(\n",
    "    source=str(c_path),\n",
    "    function_name='mnist_inference',\n",
    "    optimization='O3',\n",
    "    use_firmware_elf=True,\n",
    "    smart_args=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Loaded at: 0x{func.code_addr:08X}\")\n",
    "print(f\"✓ Binary: {func.stats['code_size']} bytes ({func.stats['code_size']/1024:.2f} KB)\")\n",
    "\n",
    "# Generate disassembly file\n",
    "disasm_path = SOURCE_DIR / \"disassembly.txt\"\n",
    "func.binary.disassemble(output=str(disasm_path), source_intermix=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INFERENCE ON ESP32-P4\n",
      "================================================================================\n",
      "\n",
      "Scale exponents:\n",
      "  Input: 5\n",
      "  Conv1: W=7, Act=4\n",
      "  Conv2: W=8, Act=4\n",
      "  FC1: W=9, Act=2\n",
      "  FC2: W=9\n",
      "\n",
      "Running inference...\n",
      "\n",
      "✓ Digit 0: Predicted 0 | 9,088,301 cycles (25.25 ms)\n",
      "✓ Digit 1: Predicted 1 | 9,089,479 cycles (25.25 ms)\n",
      "✓ Digit 2: Predicted 2 | 9,087,897 cycles (25.24 ms)\n",
      "✓ Digit 3: Predicted 3 | 9,089,017 cycles (25.25 ms)\n",
      "✓ Digit 4: Predicted 4 | 9,090,005 cycles (25.25 ms)\n",
      "✓ Digit 5: Predicted 5 | 9,088,137 cycles (25.24 ms)\n",
      "✓ Digit 6: Predicted 6 | 9,090,519 cycles (25.25 ms)\n",
      "✓ Digit 7: Predicted 7 | 9,087,877 cycles (25.24 ms)\n",
      "✓ Digit 8: Predicted 8 | 9,088,893 cycles (25.25 ms)\n",
      "✓ Digit 9: Predicted 9 | 9,089,503 cycles (25.25 ms)\n",
      "\n",
      "================================================================================\n",
      "RESULTS\n",
      "================================================================================\n",
      "✓ Accuracy: 10/10 = 100.0%\n",
      "✓ Avg cycles: 9,088,963\n",
      "✓ Avg time: 25.25 ms\n",
      "✓ Throughput: 39.6 fps\n",
      "✓ Cycles/inference: 9,088,963\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INFERENCE ON ESP32-P4\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare exponents\n",
    "e_in = scale_exponents['input']\n",
    "e_conv1_w = scale_exponents['conv1']['w']\n",
    "e_conv1_act = scale_exponents['conv1']['act']\n",
    "e_conv2_w = scale_exponents['conv2']['w']\n",
    "e_conv2_act = scale_exponents['conv2']['act']\n",
    "e_fc1_w = scale_exponents['fc1']['w']\n",
    "e_fc1_act = scale_exponents['fc1']['act']\n",
    "e_fc2_w = scale_exponents['fc2']['w']\n",
    "\n",
    "print(f\"\\nScale exponents:\")\n",
    "print(f\"  Input: {e_in}\")\n",
    "print(f\"  Conv1: W={e_conv1_w}, Act={e_conv1_act}\")\n",
    "print(f\"  Conv2: W={e_conv2_w}, Act={e_conv2_act}\")\n",
    "print(f\"  FC1: W={e_fc1_w}, Act={e_fc1_act}\")\n",
    "print(f\"  FC2: W={e_fc2_w}\")\n",
    "\n",
    "scratch = np.zeros(65536, dtype=np.int8)\n",
    "timing_buffer = np.zeros(1, dtype=np.uint32)  # ← NEW: Timing array\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"\\nRunning inference...\\n\")\n",
    "\n",
    "for digit in range(10):\n",
    "    # Reset timing buffer\n",
    "    timing_buffer[0] = 0\n",
    "    \n",
    "    predicted = func(\n",
    "        test_images_int8[digit],\n",
    "        quantized_weights['conv1']['weight'],\n",
    "        quantized_weights['conv1']['bias'],\n",
    "        quantized_weights['conv2']['weight'],\n",
    "        quantized_weights['conv2']['bias'],\n",
    "        quantized_weights['fc1']['weight'],\n",
    "        quantized_weights['fc1']['bias'],\n",
    "        quantized_weights['fc2']['weight'],\n",
    "        quantized_weights['fc2']['bias'],\n",
    "        np.int32(e_in),\n",
    "        np.int32(e_conv1_w), np.int32(e_conv1_act),\n",
    "        np.int32(e_conv2_w), np.int32(e_conv2_act),\n",
    "        np.int32(e_fc1_w), np.int32(e_fc1_act),\n",
    "        np.int32(e_fc2_w),\n",
    "        scratch,\n",
    "        timing_buffer  # ← NEW: Pass timing array\n",
    "    )\n",
    "    \n",
    "    # Read timing from P4\n",
    "    cycles = int(timing_buffer[0])\n",
    "    time_us = cycles / 360.0  # ESP32-P4 @ 360 MHz\n",
    "    time_ms = time_us / 1000.0\n",
    "    \n",
    "    results[digit] = {\n",
    "        'true': digit,\n",
    "        'predicted': predicted,\n",
    "        'correct': (predicted == digit),\n",
    "        'cycles': cycles,\n",
    "        'time_ms': time_ms\n",
    "    }\n",
    "    \n",
    "    status = \"✓\" if predicted == digit else \"✗\"\n",
    "    print(f\"{status} Digit {digit}: Predicted {predicted} | {cycles:,} cycles ({time_ms:.2f} ms)\")\n",
    "\n",
    "correct = sum(1 for r in results.values() if r['correct'])\n",
    "accuracy = 100.0 * correct / len(results)\n",
    "avg_cycles = np.mean([r['cycles'] for r in results.values()])\n",
    "avg_time = np.mean([r['time_ms'] for r in results.values()])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Accuracy: {correct}/10 = {accuracy:.1f}%\")\n",
    "print(f\"✓ Avg cycles: {avg_cycles:,.0f}\")\n",
    "print(f\"✓ Avg time: {avg_time:.2f} ms\")\n",
    "print(f\"✓ Throughput: {1000/avg_time:.1f} fps\")\n",
    "print(f\"✓ Cycles/inference: {avg_cycles:,.0f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = np.zeros((10, 10), dtype=int)\n",
    "for r in results.values():\n",
    "    confusion[r['true'], r['predicted']] += 1\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Confusion matrix\n",
    "im = ax1.imshow(confusion, cmap='Blues')\n",
    "ax1.set_xticks(np.arange(10))\n",
    "ax1.set_yticks(np.arange(10))\n",
    "ax1.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('True', fontsize=12, fontweight='bold')\n",
    "ax1.set_title(f'ESP32-P4 Results\\nAccuracy: {accuracy:.1f}%', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        ax1.text(j, i, confusion[i, j], ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if confusion[i, j] > 0 else \"black\",\n",
    "                fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax1)\n",
    "\n",
    "# Timing\n",
    "times = [results[d]['time_ms'] for d in range(10)]\n",
    "colors = ['green' if results[d]['correct'] else 'red' for d in range(10)]\n",
    "\n",
    "ax2.bar(range(10), times, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax2.axhline(avg_time, color='blue', linestyle='--', linewidth=2, label=f'Avg: {avg_time:.2f} ms')\n",
    "ax2.set_xlabel('Digit', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Time (ms)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Inference Time', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(range(10))\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL REPORT\n",
      "================================================================================\n",
      "\n",
      "📊 Model:\n",
      "  • Architecture: Conv(1→16) → Conv(16→32) → FC(1568→128) → FC(128→10)\n",
      "  • Parameters: 206,922 bytes (202.07 KB)\n",
      "  • Quantization: INT8 weights + activations + INPUT\n",
      "\n",
      "⚡ Performance:\n",
      "  • Platform: ESP32-P4 @ 360 MHz (RISC-V)\n",
      "  • Code size: 4.27 KB\n",
      "  • Inference: 25.25 ms/image\n",
      "  • Throughput: 39.6 fps\n",
      "\n",
      "🎯 Accuracy:\n",
      "  • Training (FP32): 98.48%\n",
      "  • Training (QAT): 98.79%\n",
      "  • On-device (INT8): 100.0%\n",
      "  • Retention: 101.2%\n",
      "\n",
      "🔧 Critical Fixes:\n",
      "  1. ✅ Bias scaled by E_in (not E_in + E_w - E_out)\n",
      "  2. ✅ Input quantization from start\n",
      "  3. ✅ Proper preprocessing match\n",
      "  4. ✅ Individual exponents passed to C\n",
      "  5. ✅ Smart Args for clean interface\n",
      "\n",
      "✨ P4-JIT Advantages:\n",
      "  • 2-3 second deploy (vs 30-60s firmware rebuild)\n",
      "  • No firmware changes needed\n",
      "  • Native RISC-V execution\n",
      "  • Seamless Python ↔ C workflow\n",
      "\n",
      "================================================================================\n",
      "✓ COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📊 Model:\")\n",
    "print(f\"  • Architecture: Conv(1→16) → Conv(16→32) → FC(1568→128) → FC(128→10)\")\n",
    "print(f\"  • Parameters: {total_params:,} bytes ({total_params/1024:.2f} KB)\")\n",
    "print(f\"  • Quantization: INT8 weights + activations + INPUT\")\n",
    "\n",
    "print(\"\\n⚡ Performance:\")\n",
    "print(f\"  • Platform: ESP32-P4 @ 360 MHz (RISC-V)\")\n",
    "print(f\"  • Code size: {func.stats['code_size']/1024:.2f} KB\")\n",
    "print(f\"  • Inference: {avg_time:.2f} ms/image\")\n",
    "print(f\"  • Throughput: {1000/avg_time:.1f} fps\")\n",
    "\n",
    "print(\"\\n🎯 Accuracy:\")\n",
    "print(f\"  • Training (FP32): {history['test_acc'][WARMUP_EPOCHS-1]:.2f}%\")\n",
    "print(f\"  • Training (QAT): {history['test_acc'][-1]:.2f}%\")\n",
    "print(f\"  • On-device (INT8): {accuracy:.1f}%\")\n",
    "print(f\"  • Retention: {accuracy/history['test_acc'][-1]*100:.1f}%\")\n",
    "\n",
    "print(\"\\n🔧 Critical Fixes:\")\n",
    "print(\"  1. ✅ Bias scaled by E_in (not E_in + E_w - E_out)\")\n",
    "print(\"  2. ✅ Input quantization from start\")\n",
    "print(\"  3. ✅ Proper preprocessing match\")\n",
    "print(\"  4. ✅ Individual exponents passed to C\")\n",
    "print(\"  5. ✅ Smart Args for clean interface\")\n",
    "\n",
    "print(\"\\n✨ P4-JIT Advantages:\")\n",
    "print(\"  • 2-3 second deploy (vs 30-60s firmware rebuild)\")\n",
    "print(\"  • No firmware changes needed\")\n",
    "print(\"  • Native RISC-V execution\")\n",
    "print(\"  • Seamless Python ↔ C workflow\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:38:40 [p4jit.runtime.device_manager] \u001b[94mINFO\u001b[0m: Disconnected.\n",
      "✓ Cleanup complete\n"
     ]
    }
   ],
   "source": [
    "func.free()\n",
    "jit.session.device.disconnect()\n",
    "print(\"✓ Cleanup complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
