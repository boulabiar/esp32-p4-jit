{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST INT8 Classification on ESP32-P4 using P4-JIT\n",
    "\n",
    "**Complete Quantization-Aware Training â†’ Native RISC-V Deployment Pipeline**\n",
    "\n",
    "## Features:\n",
    "- âœ… **Fake Quantization** with Straight-Through Estimator (STE)\n",
    "- âœ… **Power-of-2 Scales** for efficient bit-shift operations\n",
    "- âœ… **INT8 weights & activations** throughout\n",
    "- âœ… **INT32 accumulators** for precision\n",
    "- âœ… **Zero firmware changes** via P4-JIT\n",
    "- âœ… **Native RISC-V execution** at 360 MHz\n",
    "- âœ… **Comprehensive binary analysis**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Environment ready\n",
      "âœ“ PyTorch version: 2.8.0+cu126\n",
      "âœ“ Device: CUDA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import struct\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Setup directories\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "SOURCE_DIR = NOTEBOOK_DIR / \"source\"\n",
    "WEIGHTS_DIR = NOTEBOOK_DIR / \"weights\"\n",
    "RESULTS_DIR = NOTEBOOK_DIR / \"results\"\n",
    "\n",
    "for d in [SOURCE_DIR, WEIGHTS_DIR, RESULTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add P4-JIT to path\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent.parent\n",
    "sys.path.append(str(PROJECT_ROOT / \"host\"))\n",
    "\n",
    "from p4jit import P4JIT, MALLOC_CAP_SPIRAM, MALLOC_CAP_8BIT\n",
    "import p4jit\n",
    "\n",
    "# Configure\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ“ Environment ready\")\n",
    "print(f\"âœ“ PyTorch version: {torch.__version__}\")\n",
    "print(f\"âœ“ Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "print(f\"âœ“ Training samples: {len(train_dataset):,}\")\n",
    "print(f\"âœ“ Test samples: {len(test_dataset):,}\")\n",
    "\n",
    "# Visualize samples\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n",
    "for i in range(20):\n",
    "    img, label = train_dataset[i]\n",
    "    ax = axes[i // 10, i % 10]\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.set_title(f'{label}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('MNIST Dataset Samples', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'dataset_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quantization Module with Straight-Through Estimator\n",
    "\n",
    "**Key Features:**\n",
    "- Power-of-2 scales (2^n) for efficient bit-shift operations\n",
    "- Straight-Through Estimator for gradient flow\n",
    "- **CONSTANT scale exponents** (determined via calibration)\n",
    "- Quantize both weights AND activations during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Quantization modules defined\n",
      "âœ“ Using Straight-Through Estimator for gradient flow\n",
      "âœ“ Power-of-2 scales for efficient bit-shift operations\n",
      "âœ“ Scale exponents will be CALIBRATED and FROZEN\n"
     ]
    }
   ],
   "source": [
    "class PowerOfTwoQuantize(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Straight-Through Estimator for INT8 quantization.\n",
    "    Uses power-of-2 scale for efficient bit-shift operations.\n",
    "    \n",
    "    Forward: Quantize to INT8 using scale = 2^(-n)\n",
    "    Backward: Pass gradients straight through (STE)\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, scale_exp):\n",
    "        # scale = 2^(-scale_exp), so quantization is x * 2^scale_exp\n",
    "        # Then clip to INT8 range [-128, 127]\n",
    "        scale = 2.0 ** scale_exp\n",
    "        x_scaled = x * scale\n",
    "        x_quant = torch.clamp(torch.round(x_scaled), -128, 127)\n",
    "        x_dequant = x_quant / scale\n",
    "        return x_dequant\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Straight-through: gradient flows unchanged\n",
    "        return grad_output, None\n",
    "\n",
    "\n",
    "class FakeQuantizeINT8(nn.Module):\n",
    "    \"\"\"\n",
    "    Fake quantization module with FIXED power-of-2 scale.\n",
    "    Scale exponent is calibrated and then frozen.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # NOT a Parameter - just a buffer (constant)\n",
    "        self.register_buffer('scale_exp', torch.tensor(0))\n",
    "        self.enabled = False  # Start disabled\n",
    "        \n",
    "    def set_scale_exp(self, exp):\n",
    "        \"\"\"Set the fixed scale exponent after calibration\"\"\"\n",
    "        self.scale_exp = torch.tensor(exp)\n",
    "        \n",
    "    def enable(self):\n",
    "        \"\"\"Enable quantization (after calibration)\"\"\"\n",
    "        self.enabled = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not self.enabled:\n",
    "            # Quantization disabled - pass through\n",
    "            return x\n",
    "        \n",
    "        # Use fixed scale_exp\n",
    "        return PowerOfTwoQuantize.apply(x, self.scale_exp)\n",
    "    \n",
    "    def get_scale_info(self):\n",
    "        \"\"\"Get quantization scale as power-of-2\"\"\"\n",
    "        exp = int(self.scale_exp.item())\n",
    "        scale = 2.0 ** (-exp)\n",
    "        return exp, scale\n",
    "\n",
    "\n",
    "class QuantizedConv2d(nn.Module):\n",
    "    \"\"\"Conv2d with weight quantization\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, **kwargs)\n",
    "        self.weight_quant = FakeQuantizeINT8()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Quantize weights (if enabled)\n",
    "        w_quant = self.weight_quant(self.conv.weight)\n",
    "        # Use quantized weights for convolution\n",
    "        return F.conv2d(x, w_quant, self.conv.bias, \n",
    "                       self.conv.stride, self.conv.padding, \n",
    "                       self.conv.dilation, self.conv.groups)\n",
    "\n",
    "\n",
    "class QuantizedLinear(nn.Module):\n",
    "    \"\"\"Linear layer with weight quantization\"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.weight_quant = FakeQuantizeINT8()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Quantize weights (if enabled)\n",
    "        w_quant = self.weight_quant(self.linear.weight)\n",
    "        return F.linear(x, w_quant, self.linear.bias)\n",
    "\n",
    "\n",
    "print(\"âœ“ Quantization modules defined\")\n",
    "print(\"âœ“ Using Straight-Through Estimator for gradient flow\")\n",
    "print(\"âœ“ Power-of-2 scales for efficient bit-shift operations\")\n",
    "print(\"âœ“ Scale exponents will be CALIBRATED and FROZEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quantized Neural Network Architecture\n",
    "\n",
    "**Network Structure:**\n",
    "```\n",
    "Input (28Ã—28) \n",
    "  â†’ Conv2d(1â†’16, 3Ã—3) â†’ Quantize â†’ ReLU â†’ Quantize â†’ MaxPool(2Ã—2)\n",
    "  â†’ Conv2d(16â†’32, 3Ã—3) â†’ Quantize â†’ ReLU â†’ Quantize â†’ MaxPool(2Ã—2)\n",
    "  â†’ Flatten\n",
    "  â†’ Linear(800â†’128) â†’ Quantize â†’ ReLU â†’ Quantize\n",
    "  â†’ Linear(128â†’10)\n",
    "  â†’ Output (10 classes)\n",
    "```\n",
    "\n",
    "**All weights and activations are fake-quantized to INT8!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Model created\n",
      "âœ“ Total parameters: 206,922\n",
      "âœ“ Trainable parameters: 206,922\n",
      "\n",
      "Architecture:\n",
      "QuantizedMNISTNet(\n",
      "  (conv1): QuantizedConv2d(\n",
      "    (conv): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (weight_quant): FakeQuantizeINT8()\n",
      "  )\n",
      "  (act1_quant): FakeQuantizeINT8()\n",
      "  (conv2): QuantizedConv2d(\n",
      "    (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (weight_quant): FakeQuantizeINT8()\n",
      "  )\n",
      "  (act2_quant): FakeQuantizeINT8()\n",
      "  (fc1): QuantizedLinear(\n",
      "    (linear): Linear(in_features=1568, out_features=128, bias=True)\n",
      "    (weight_quant): FakeQuantizeINT8()\n",
      "  )\n",
      "  (act3_quant): FakeQuantizeINT8()\n",
      "  (fc2): QuantizedLinear(\n",
      "    (linear): Linear(in_features=128, out_features=10, bias=True)\n",
      "    (weight_quant): FakeQuantizeINT8()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class QuantizedMNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Layer 1: Conv + ReLU + MaxPool\n",
    "        self.conv1 = QuantizedConv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.act1_quant = FakeQuantizeINT8()\n",
    "        \n",
    "        # Layer 2: Conv + ReLU + MaxPool\n",
    "        self.conv2 = QuantizedConv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.act2_quant = FakeQuantizeINT8()\n",
    "        \n",
    "        # Layer 3: FC + ReLU\n",
    "        self.fc1 = QuantizedLinear(32 * 7 * 7, 128)\n",
    "        self.act3_quant = FakeQuantizeINT8()\n",
    "        \n",
    "        # Layer 4: FC (output)\n",
    "        self.fc2 = QuantizedLinear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Conv1 block\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.act1_quant(x)  # Quantize activations\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Conv2 block\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.act2_quant(x)  # Quantize activations\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC1 block\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.act3_quant(x)  # Quantize activations\n",
    "        \n",
    "        # FC2 (output logits - no quantization)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def print_quantization_info(self):\n",
    "        \"\"\"Print quantization scale information for all layers\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"QUANTIZATION PARAMETERS (Power-of-2 Scales)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, FakeQuantizeINT8):\n",
    "                exp, scale = module.get_scale_info()\n",
    "                print(f\"{name:30s} | Scale: 2^({exp:+3d}) = {scale:.6f} | Shift: {exp} bits\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = QuantizedMNISTNet().to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nâœ“ Model created\")\n",
    "print(f\"âœ“ Total parameters: {total_params:,}\")\n",
    "print(f\"âœ“ Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Pipeline: Warmup â†’ Calibration â†’ QAT\n",
    "\n",
    "**3-Phase Training:**\n",
    "1. **Warmup (3 epochs)**: Train without quantization to stabilize weights\n",
    "2. **Calibration**: Determine optimal scale exponents using statistics\n",
    "3. **QAT (7 epochs)**: Train with FIXED quantization scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 1: WARMUP TRAINING (No Quantization)\n",
      "================================================================================\n",
      "Training without quantization to stabilize weights...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf9d76929d34e12b260db1c9f508fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup Epoch 1/3 | Train Loss: 0.0378 Acc: 98.87% | Test Loss: 0.0336 Acc: 98.94%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa26d41cd1f4be59591f08d9bf7d452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup Epoch 2/3 | Train Loss: 0.0258 Acc: 99.19% | Test Loss: 0.0414 Acc: 98.65%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1dc09e817c47bcb8871e7c9a1e7772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup Epoch 3/3 | Train Loss: 0.0207 Acc: 99.34% | Test Loss: 0.0339 Acc: 98.83%\n",
      "\n",
      "âœ“ Warmup complete! Weights stabilized.\n",
      "âœ“ Warmup Test Accuracy: 98.83%\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data, target in tqdm(loader, desc='Training', leave=False):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "# Training configuration\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training phases\n",
    "WARMUP_EPOCHS = 3\n",
    "QAT_EPOCHS = 3\n",
    "TOTAL_EPOCHS = WARMUP_EPOCHS + QAT_EPOCHS\n",
    "\n",
    "history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1: WARMUP TRAINING (No Quantization)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Training without quantization to stabilize weights...\\n\")\n",
    "\n",
    "for epoch in range(1, WARMUP_EPOCHS + 1):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    print(f\"Warmup Epoch {epoch}/{WARMUP_EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | \"\n",
    "          f\"Test Loss: {test_loss:.4f} Acc: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"\\nâœ“ Warmup complete! Weights stabilized.\")\n",
    "print(f\"âœ“ Warmup Test Accuracy: {history['test_acc'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration: Determine Scale Exponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 2: CALIBRATION\n",
      "================================================================================\n",
      "Calculating optimal scale exponents...\n",
      "\n",
      "Weight Calibration:\n",
      "  conv1                | Exponent:  +7 | Scale: 2^( +7) = 0.007812\n",
      "  conv2                | Exponent:  +8 | Scale: 2^( +8) = 0.003906\n",
      "  fc1                  | Exponent:  +8 | Scale: 2^( +8) = 0.003906\n",
      "  fc2                  | Exponent:  +8 | Scale: 2^( +8) = 0.003906\n",
      "\n",
      "Activation Calibration (using 10 batches):\n",
      "  act1_quant           | Exponent:  +2 | Scale: 2^( +2) = 0.250000\n",
      "  act2_quant           | Exponent:  +2 | Scale: 2^( +2) = 0.250000\n",
      "  act3_quant           | Exponent:  +2 | Scale: 2^( +2) = 0.250000\n",
      "\n",
      "âœ“ Calibration complete! Scale exponents are now FIXED.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def calculate_scale_exponent(tensor):\n",
    "    \"\"\"\n",
    "    Calculate optimal power-of-2 scale exponent for INT8 quantization.\n",
    "    \n",
    "    For a tensor to fit in [-128, 127]:\n",
    "    max(abs(tensor)) * 2^exp <= 127\n",
    "    exp = floor(log2(127 / max(abs(tensor))))\n",
    "    \"\"\"\n",
    "    max_val = tensor.abs().max().item()\n",
    "    if max_val == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate exponent: scale = 2^exp such that max_val * 2^exp <= 127\n",
    "    import math\n",
    "    exp = math.floor(math.log2(127.0 / max_val))\n",
    "    return exp\n",
    "\n",
    "\n",
    "def calibrate_activations(model, loader, device, num_batches=10):\n",
    "    \"\"\"\n",
    "    Calibrate activation scales using statistics from multiple batches.\n",
    "    Returns a dict mapping layer names to scale exponents.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Hooks to capture activations\n",
    "    activation_stats = {}\n",
    "    hooks = []\n",
    "    \n",
    "    def get_hook(name):\n",
    "        def hook(module, input, output):\n",
    "            if name not in activation_stats:\n",
    "                activation_stats[name] = []\n",
    "            activation_stats[name].append(output.abs().max().item())\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks for activation quantizers\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, FakeQuantizeINT8) and 'act' in name:\n",
    "            # Hook on the layer BEFORE the quantizer\n",
    "            parent_name = '.'.join(name.split('.')[:-1])\n",
    "            if parent_name:\n",
    "                parent_module = dict(model.named_modules())[parent_name]\n",
    "            else:\n",
    "                parent_module = model\n",
    "            \n",
    "            hooks.append(parent_module.register_forward_hook(get_hook(name)))\n",
    "    \n",
    "    # Run inference on calibration batches\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(loader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "            data = data.to(device)\n",
    "            model(data)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    # Calculate exponents from statistics\n",
    "    exponents = {}\n",
    "    for name, values in activation_stats.items():\n",
    "        max_activation = max(values)\n",
    "        import math\n",
    "        exp = math.floor(math.log2(127.0 / max_activation)) if max_activation > 0 else 0\n",
    "        exponents[name] = exp\n",
    "    \n",
    "    return exponents\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2: CALIBRATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"Calculating optimal scale exponents...\\n\")\n",
    "\n",
    "# Calibrate weights (simple - based on current weight values)\n",
    "print(\"Weight Calibration:\")\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, (QuantizedConv2d, QuantizedLinear)):\n",
    "        if isinstance(module, QuantizedConv2d):\n",
    "            weight = module.conv.weight\n",
    "        else:\n",
    "            weight = module.linear.weight\n",
    "        \n",
    "        exp = calculate_scale_exponent(weight.data)\n",
    "        module.weight_quant.set_scale_exp(exp)\n",
    "        \n",
    "        scale = 2.0 ** (-exp)\n",
    "        print(f\"  {name:20s} | Exponent: {exp:+3d} | Scale: 2^({exp:+3d}) = {scale:.6f}\")\n",
    "\n",
    "# Calibrate activations (using statistics from large batch)\n",
    "print(\"\\nActivation Calibration (using 10 batches):\")\n",
    "act_exponents = calibrate_activations(model, train_loader, device, num_batches=10)\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, FakeQuantizeINT8) and name in act_exponents:\n",
    "        exp = act_exponents[name]\n",
    "        module.set_scale_exp(exp)\n",
    "        scale = 2.0 ** (-exp)\n",
    "        print(f\"  {name:20s} | Exponent: {exp:+3d} | Scale: 2^({exp:+3d}) = {scale:.6f}\")\n",
    "\n",
    "print(\"\\nâœ“ Calibration complete! Scale exponents are now FIXED.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Quantization and Continue Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 3: QUANTIZATION-AWARE TRAINING (Fixed Scales)\n",
      "================================================================================\n",
      "Training with fake quantization enabled...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e5533300c14485978fdf7697cbeedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAT Epoch 1/3 (Total: 4/6) | Train Loss: 0.0175 Acc: 99.42% | Test Loss: 0.0330 Acc: 99.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23c65f2f61c4d5e90420e1bc2121443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAT Epoch 2/3 (Total: 5/6) | Train Loss: 0.0121 Acc: 99.61% | Test Loss: 0.0296 Acc: 99.07%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d863f8502846a6961f6e209827981f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAT Epoch 3/3 (Total: 6/6) | Train Loss: 0.0096 Acc: 99.72% | Test Loss: 0.0420 Acc: 98.80%\n",
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE\n",
      "================================================================================\n",
      "âœ“ Final Test Accuracy: 98.80%\n",
      "âœ“ Accuracy after warmup: 98.83%\n",
      "âœ“ Accuracy after QAT: 98.80%\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "QUANTIZATION PARAMETERS (Power-of-2 Scales)\n",
      "================================================================================\n",
      "conv1.weight_quant             | Scale: 2^( +7) = 0.007812 | Shift: 7 bits\n",
      "act1_quant                     | Scale: 2^( +2) = 0.250000 | Shift: 2 bits\n",
      "conv2.weight_quant             | Scale: 2^( +8) = 0.003906 | Shift: 8 bits\n",
      "act2_quant                     | Scale: 2^( +2) = 0.250000 | Shift: 2 bits\n",
      "fc1.weight_quant               | Scale: 2^( +8) = 0.003906 | Shift: 8 bits\n",
      "act3_quant                     | Scale: 2^( +2) = 0.250000 | Shift: 2 bits\n",
      "fc2.weight_quant               | Scale: 2^( +8) = 0.003906 | Shift: 8 bits\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Enable all quantizers\n",
    "for module in model.modules():\n",
    "    if isinstance(module, FakeQuantizeINT8):\n",
    "        module.enable()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 3: QUANTIZATION-AWARE TRAINING (Fixed Scales)\")\n",
    "print(\"=\"*80)\n",
    "print(\"Training with fake quantization enabled...\\n\")\n",
    "\n",
    "# Reset optimizer for QAT phase\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "for epoch in range(1, QAT_EPOCHS + 1):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    total_epoch = WARMUP_EPOCHS + epoch\n",
    "    print(f\"QAT Epoch {epoch}/{QAT_EPOCHS} (Total: {total_epoch}/{TOTAL_EPOCHS}) | \"\n",
    "          f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | \"\n",
    "          f\"Test Loss: {test_loss:.4f} Acc: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"âœ“ Final Test Accuracy: {history['test_acc'][-1]:.2f}%\")\n",
    "print(f\"âœ“ Accuracy after warmup: {history['test_acc'][WARMUP_EPOCHS-1]:.2f}%\")\n",
    "print(f\"âœ“ Accuracy after QAT: {history['test_acc'][-1]:.2f}%\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show final quantization parameters\n",
    "model.print_quantization_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history with phase separation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs = list(range(1, len(history['train_loss']) + 1))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(epochs, history['train_loss'], label='Train Loss', marker='o', linewidth=2)\n",
    "ax1.plot(epochs, history['test_loss'], label='Test Loss', marker='s', linewidth=2)\n",
    "ax1.axvline(x=WARMUP_EPOCHS, color='red', linestyle='--', linewidth=2, \n",
    "            label='Calibration Point')\n",
    "ax1.fill_between([0, WARMUP_EPOCHS], 0, ax1.get_ylim()[1], alpha=0.2, color='orange', \n",
    "                  label='Warmup Phase')\n",
    "ax1.fill_between([WARMUP_EPOCHS, len(epochs)], 0, ax1.get_ylim()[1], alpha=0.2, color='blue',\n",
    "                  label='QAT Phase')\n",
    "ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10, loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(epochs, history['train_acc'], label='Train Accuracy', marker='o', linewidth=2)\n",
    "ax2.plot(epochs, history['test_acc'], label='Test Accuracy', marker='s', linewidth=2)\n",
    "ax2.axvline(x=WARMUP_EPOCHS, color='red', linestyle='--', linewidth=2,\n",
    "            label='Calibration Point')\n",
    "ax2.fill_between([0, WARMUP_EPOCHS], 0, 100, alpha=0.2, color='orange',\n",
    "                  label='Warmup Phase')\n",
    "ax2.fill_between([WARMUP_EPOCHS, len(epochs)], 0, 100, alpha=0.2, color='blue',\n",
    "                  label='QAT Phase')\n",
    "ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Training Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10, loc='lower right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Training Summary:\")\n",
    "print(f\"  Warmup Phase: {WARMUP_EPOCHS} epochs (no quantization)\")\n",
    "print(f\"  Calibration: Scale exponents determined and frozen\")\n",
    "print(f\"  QAT Phase: {QAT_EPOCHS} epochs (with fixed quantization)\")\n",
    "print(f\"  Final Accuracy: {history['test_acc'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract Quantized Weights (INT8 + Power-of-2 Scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "WEIGHT EXTRACTION & QUANTIZATION\n",
      "================================================================================\n",
      "Conv1: W(16, 1, 3, 3) [-102,  +70] | Scale: 2^+7\n",
      "Conv2: W(32, 16, 3, 3) [-111,  +91] | Scale: 2^+8\n",
      "FC1:   W(128, 1568) [-109,  +83] | Scale: 2^+8\n",
      "FC2:   W(10, 128) [ -88,  +59] | Scale: 2^+8\n",
      "\n",
      "âœ“ Total INT8 parameters: 206,922 bytes (202.07 KB)\n",
      "âœ“ Equivalent FP32 size: 808.29 KB\n",
      "âœ“ Compression ratio: 4.0Ã—\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def quantize_to_int8(tensor, scale_exp):\n",
    "    \"\"\"\n",
    "    Quantize tensor to INT8 using power-of-2 scale.\n",
    "    scale_exp: scale = 2^(-scale_exp)\n",
    "    \"\"\"\n",
    "    scale = 2.0 ** scale_exp\n",
    "    quantized = torch.clamp(torch.round(tensor * scale), -128, 127).to(torch.int8)\n",
    "    return quantized.cpu().numpy(), scale_exp\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "quantized_weights = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WEIGHT EXTRACTION & QUANTIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Conv1\n",
    "exp, _ = model.conv1.weight_quant.get_scale_info()\n",
    "w, scale_exp = quantize_to_int8(model.conv1.conv.weight.data, exp)\n",
    "b, bias_exp = quantize_to_int8(model.conv1.conv.bias.data, exp)\n",
    "\n",
    "quantized_weights['conv1'] = {\n",
    "    'weight': w, 'weight_shape': w.shape, 'weight_scale_exp': scale_exp,\n",
    "    'bias': b, 'bias_shape': b.shape, 'bias_scale_exp': bias_exp\n",
    "}\n",
    "print(f\"Conv1: W{w.shape} [{w.min():+4d}, {w.max():+4d}] | Scale: 2^{scale_exp:+d}\")\n",
    "\n",
    "# Conv2\n",
    "exp, _ = model.conv2.weight_quant.get_scale_info()\n",
    "w, scale_exp = quantize_to_int8(model.conv2.conv.weight.data, exp)\n",
    "b, bias_exp = quantize_to_int8(model.conv2.conv.bias.data, exp)\n",
    "\n",
    "quantized_weights['conv2'] = {\n",
    "    'weight': w, 'weight_shape': w.shape, 'weight_scale_exp': scale_exp,\n",
    "    'bias': b, 'bias_shape': b.shape, 'bias_scale_exp': bias_exp\n",
    "}\n",
    "print(f\"Conv2: W{w.shape} [{w.min():+4d}, {w.max():+4d}] | Scale: 2^{scale_exp:+d}\")\n",
    "\n",
    "# FC1\n",
    "exp, _ = model.fc1.weight_quant.get_scale_info()\n",
    "w, scale_exp = quantize_to_int8(model.fc1.linear.weight.data, exp)\n",
    "b, bias_exp = quantize_to_int8(model.fc1.linear.bias.data, exp)\n",
    "\n",
    "quantized_weights['fc1'] = {\n",
    "    'weight': w, 'weight_shape': w.shape, 'weight_scale_exp': scale_exp,\n",
    "    'bias': b, 'bias_shape': b.shape, 'bias_scale_exp': bias_exp\n",
    "}\n",
    "print(f\"FC1:   W{w.shape} [{w.min():+4d}, {w.max():+4d}] | Scale: 2^{scale_exp:+d}\")\n",
    "\n",
    "# FC2\n",
    "exp, _ = model.fc2.weight_quant.get_scale_info()\n",
    "w, scale_exp = quantize_to_int8(model.fc2.linear.weight.data, exp)\n",
    "b, bias_exp = quantize_to_int8(model.fc2.linear.bias.data, exp)\n",
    "\n",
    "quantized_weights['fc2'] = {\n",
    "    'weight': w, 'weight_shape': w.shape, 'weight_scale_exp': scale_exp,\n",
    "    'bias': b, 'bias_shape': b.shape, 'bias_scale_exp': bias_exp\n",
    "}\n",
    "print(f\"FC2:   W{w.shape} [{w.min():+4d}, {w.max():+4d}] | Scale: 2^{scale_exp:+d}\")\n",
    "\n",
    "# Calculate total memory\n",
    "total_int8_params = sum(w['weight'].size + w['bias'].size for w in quantized_weights.values())\n",
    "total_fp32_params = total_int8_params * 4\n",
    "\n",
    "print(f\"\\nâœ“ Total INT8 parameters: {total_int8_params:,} bytes ({total_int8_params/1024:.2f} KB)\")\n",
    "print(f\"âœ“ Equivalent FP32 size: {total_fp32_params/1024:.2f} KB\")\n",
    "print(f\"âœ“ Compression ratio: {total_fp32_params/total_int8_params:.1f}Ã—\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Optimized C Implementation\n",
    "\n",
    "**Pure INT8 Operations:**\n",
    "- All weights: INT8\n",
    "- All activations: INT8\n",
    "- Accumulators: INT32 (for precision)\n",
    "- Scaling: Bit-shift operations (power-of-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ C inference code saved: c:\\Users\\orani\\bilel\\git_projects\\robert_manzke\\project1\\trys\\costume_p4code_binary\\P4-JIT\\notebooks\\tutorials\\t02_mnist_classification\\source\\mnist_inference.c\n",
      "âœ“ Code size: 6524 bytes\n",
      "âœ“ All operations use INT8 with INT32 accumulators\n",
      "âœ“ Scaling via bit-shift (power-of-2 scales)\n"
     ]
    }
   ],
   "source": [
    "c_inference_code = \"\"\"#include <stdint.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "// Network dimensions\n",
    "#define INPUT_H 28\n",
    "#define INPUT_W 28\n",
    "#define CONV1_OUT_C 16\n",
    "#define CONV2_OUT_C 32\n",
    "#define FC1_OUT 128\n",
    "#define OUTPUT_SIZE 10\n",
    "\n",
    "// INT8 operations\n",
    "static inline int8_t relu_int8(int8_t x) {\n",
    "    return (x > 0) ? x : 0;\n",
    "}\n",
    "\n",
    "static inline int8_t clip_int8(int32_t x) {\n",
    "    if (x > 127) return 127;\n",
    "    if (x < -128) return -128;\n",
    "    return (int8_t)x;\n",
    "}\n",
    "\n",
    "// 2D Convolution: INT8 weights, INT8 input, INT32 accumulator, INT8 output\n",
    "// Uses bit-shift for scaling (power-of-2)\n",
    "void conv2d_int8(\n",
    "    const int8_t* input, int in_h, int in_w, int in_c,\n",
    "    const int8_t* weight, const int8_t* bias,\n",
    "    int8_t* output, int out_c,\n",
    "    int scale_shift  // Power-of-2: divide by 2^scale_shift\n",
    ") {\n",
    "    const int kernel_size = 3;\n",
    "    const int padding = 1;\n",
    "    const int stride = 1;\n",
    "    \n",
    "    int out_h = (in_h + 2*padding - kernel_size) / stride + 1;\n",
    "    int out_w = (in_w + 2*padding - kernel_size) / stride + 1;\n",
    "    \n",
    "    for (int oc = 0; oc < out_c; oc++) {\n",
    "        for (int oh = 0; oh < out_h; oh++) {\n",
    "            for (int ow = 0; ow < out_w; ow++) {\n",
    "                int32_t acc = 0;  // INT32 accumulator for precision\n",
    "                \n",
    "                // Convolution (INT8 Ã— INT8 â†’ INT32)\n",
    "                for (int ic = 0; ic < in_c; ic++) {\n",
    "                    for (int kh = 0; kh < kernel_size; kh++) {\n",
    "                        for (int kw = 0; kw < kernel_size; kw++) {\n",
    "                            int ih = oh * stride - padding + kh;\n",
    "                            int iw = ow * stride - padding + kw;\n",
    "                            \n",
    "                            if (ih >= 0 && ih < in_h && iw >= 0 && iw < in_w) {\n",
    "                                int in_idx = (ic * in_h + ih) * in_w + iw;\n",
    "                                int w_idx = ((oc * in_c + ic) * kernel_size + kh) * kernel_size + kw;\n",
    "                                acc += (int32_t)input[in_idx] * (int32_t)weight[w_idx];\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                // Add bias (INT32)\n",
    "                acc += (int32_t)bias[oc] << scale_shift;\n",
    "                \n",
    "                // Scale down using bit-shift (power-of-2)\n",
    "                acc = acc >> scale_shift;\n",
    "                \n",
    "                // Clip to INT8 range\n",
    "                int out_idx = (oc * out_h + oh) * out_w + ow;\n",
    "                output[out_idx] = clip_int8(acc);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// MaxPool 2x2\n",
    "void maxpool2d_int8(const int8_t* input, int8_t* output, int h, int w, int c) {\n",
    "    int out_h = h / 2;\n",
    "    int out_w = w / 2;\n",
    "    \n",
    "    for (int ch = 0; ch < c; ch++) {\n",
    "        for (int oh = 0; oh < out_h; oh++) {\n",
    "            for (int ow = 0; ow < out_w; ow++) {\n",
    "                int ih = oh * 2;\n",
    "                int iw = ow * 2;\n",
    "                \n",
    "                int8_t max_val = -128;\n",
    "                for (int kh = 0; kh < 2; kh++) {\n",
    "                    for (int kw = 0; kw < 2; kw++) {\n",
    "                        int in_idx = (ch * h + ih + kh) * w + iw + kw;\n",
    "                        if (input[in_idx] > max_val) {\n",
    "                            max_val = input[in_idx];\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                int out_idx = (ch * out_h + oh) * out_w + ow;\n",
    "                output[out_idx] = max_val;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// Fully Connected: INT8 weights, INT8 input, INT32 accumulator, INT8 output\n",
    "void fc_int8(\n",
    "    const int8_t* input, int in_size,\n",
    "    const int8_t* weight, const int8_t* bias,\n",
    "    int8_t* output, int out_size,\n",
    "    int scale_shift\n",
    ") {\n",
    "    for (int i = 0; i < out_size; i++) {\n",
    "        int32_t acc = 0;  // INT32 accumulator\n",
    "        \n",
    "        // Multiply-accumulate (INT8 Ã— INT8 â†’ INT32)\n",
    "        for (int j = 0; j < in_size; j++) {\n",
    "            acc += (int32_t)input[j] * (int32_t)weight[i * in_size + j];\n",
    "        }\n",
    "        \n",
    "        // Add bias\n",
    "        acc += (int32_t)bias[i] << scale_shift;\n",
    "        \n",
    "        // Scale down\n",
    "        acc = acc >> scale_shift;\n",
    "        \n",
    "        // Clip and store\n",
    "        output[i] = clip_int8(acc);\n",
    "    }\n",
    "}\n",
    "\n",
    "// Main inference function\n",
    "// Arguments:\n",
    "// - input: 784 bytes (28Ã—28)\n",
    "// - weights_conv1: Conv1 weights\n",
    "// - bias_conv1: Conv1 biases\n",
    "// - weights_conv2: Conv2 weights\n",
    "// - bias_conv2: Conv2 biases\n",
    "// - weights_fc1: FC1 weights\n",
    "// - bias_fc1: FC1 biases\n",
    "// - weights_fc2: FC2 weights\n",
    "// - bias_fc2: FC2 biases\n",
    "// - scratch: Scratch buffer for intermediate results\n",
    "int32_t mnist_inference(\n",
    "    int8_t* input,\n",
    "    int8_t* weights_conv1,\n",
    "    int8_t* bias_conv1,\n",
    "    int8_t* weights_conv2,\n",
    "    int8_t* bias_conv2,\n",
    "    int8_t* weights_fc1,\n",
    "    int8_t* bias_fc1,\n",
    "    int8_t* weights_fc2,\n",
    "    int8_t* bias_fc2,\n",
    "    int8_t* scratch\n",
    ") {\n",
    "    printf(\"[JIT] Starting MNIST inference...\\\\n\");\n",
    "    \n",
    "    // Allocate intermediate buffers from scratch\n",
    "    int8_t* conv1_out = scratch;\n",
    "    int8_t* pool1_out = conv1_out + (16 * 28 * 28);\n",
    "    int8_t* conv2_out = pool1_out + (16 * 14 * 14);\n",
    "    int8_t* pool2_out = conv2_out + (32 * 14 * 14);\n",
    "    int8_t* fc1_out = pool2_out + (32 * 7 * 7);\n",
    "    int8_t* fc2_out = fc1_out + 128;\n",
    "    \n",
    "    // Layer 1: Conv2d(1â†’16) + ReLU + MaxPool\n",
    "    printf(\"[JIT] Conv1...\\\\n\");\n",
    "    conv2d_int8(input, 28, 28, 1, weights_conv1, bias_conv1, conv1_out, 16, 0);\n",
    "    \n",
    "    // ReLU in-place\n",
    "    for (int i = 0; i < 16 * 28 * 28; i++) {\n",
    "        conv1_out[i] = relu_int8(conv1_out[i]);\n",
    "    }\n",
    "    \n",
    "    maxpool2d_int8(conv1_out, pool1_out, 28, 28, 16);\n",
    "    \n",
    "    // Layer 2: Conv2d(16â†’32) + ReLU + MaxPool\n",
    "    printf(\"[JIT] Conv2...\\\\n\");\n",
    "    conv2d_int8(pool1_out, 14, 14, 16, weights_conv2, bias_conv2, conv2_out, 32, 0);\n",
    "    \n",
    "    // ReLU in-place\n",
    "    for (int i = 0; i < 32 * 14 * 14; i++) {\n",
    "        conv2_out[i] = relu_int8(conv2_out[i]);\n",
    "    }\n",
    "    \n",
    "    maxpool2d_int8(conv2_out, pool2_out, 14, 14, 32);\n",
    "    \n",
    "    // Layer 3: FC(800â†’128) + ReLU\n",
    "    printf(\"[JIT] FC1...\\\\n\");\n",
    "    fc_int8(pool2_out, 800, weights_fc1, bias_fc1, fc1_out, 128, 0);\n",
    "    \n",
    "    // ReLU in-place\n",
    "    for (int i = 0; i < 128; i++) {\n",
    "        fc1_out[i] = relu_int8(fc1_out[i]);\n",
    "    }\n",
    "    \n",
    "    // Layer 4: FC(128â†’10) - Output logits\n",
    "    printf(\"[JIT] FC2...\\\\n\");\n",
    "    fc_int8(fc1_out, 128, weights_fc2, bias_fc2, fc2_out, 10, 0);\n",
    "    \n",
    "    // Find argmax\n",
    "    int8_t max_val = fc2_out[0];\n",
    "    int32_t max_idx = 0;\n",
    "    \n",
    "    for (int i = 1; i < 10; i++) {\n",
    "        if (fc2_out[i] > max_val) {\n",
    "            max_val = fc2_out[i];\n",
    "            max_idx = i;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    printf(\"[JIT] Predicted class: %d (logit: %d)\\\\n\", max_idx, max_val);\n",
    "    \n",
    "    return max_idx;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Save C code\n",
    "c_path = SOURCE_DIR / \"mnist_inference.c\"\n",
    "with open(c_path, 'w') as f:\n",
    "    f.write(c_inference_code)\n",
    "\n",
    "print(f\"âœ“ C inference code saved: {c_path}\")\n",
    "print(f\"âœ“ Code size: {len(c_inference_code)} bytes\")\n",
    "print(f\"âœ“ All operations use INT8 with INT32 accumulators\")\n",
    "print(f\"âœ“ Scaling via bit-shift (power-of-2 scales)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prepare Test Images (One Per Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one test image per class\n",
    "test_images_torch = {}\n",
    "test_images_int8 = {}\n",
    "\n",
    "for digit in range(10):\n",
    "    for idx, (img, label) in enumerate(test_dataset):\n",
    "        if label == digit:\n",
    "            test_images_torch[digit] = img\n",
    "            \n",
    "            # Convert to INT8 for device\n",
    "            img_np = img.squeeze().numpy()\n",
    "            # Normalize to [0, 1] then scale to INT8 range\n",
    "            img_norm = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "            img_int8 = ((img_norm * 255) - 128).astype(np.int8)\n",
    "            test_images_int8[digit] = img_int8.flatten()\n",
    "            break\n",
    "\n",
    "print(\"âœ“ Test images selected (one per class)\")\n",
    "print(f\"  INT8 shape: {test_images_int8[0].shape}\")\n",
    "print(f\"  INT8 range: [{test_images_int8[0].min()}, {test_images_int8[0].max()}]\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for digit in range(10):\n",
    "    ax = axes[digit // 5, digit % 5]\n",
    "    img = test_images_torch[digit].squeeze()\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f'Digit: {digit}', fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Test Images (INT8 Quantized)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'test_images.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Deploy to ESP32-P4 via P4-JIT\n",
    "\n",
    "**ðŸš€ Dynamic Code Loading Without Firmware Changes!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ESP32-P4 DEPLOYMENT\n",
      "================================================================================\n",
      "08:55:19 [p4jit.p4jit] \u001b[94mINFO\u001b[0m: Initializing P4JIT System...\n",
      "08:55:19 [p4jit.runtime.jit_session] \u001b[94mINFO\u001b[0m: Auto-detecting JIT device...\n",
      "08:55:19 [p4jit.runtime.device_manager] \u001b[94mINFO\u001b[0m: Connecting to COM3 at 115200 baud...\n",
      "08:55:19 [p4jit.runtime.device_manager] \u001b[94mINFO\u001b[0m: Connecting to COM6 at 115200 baud...\n",
      "08:55:19 [p4jit.runtime.device_manager] \u001b[94mINFO\u001b[0m: Connected.\n",
      "08:55:19 [p4jit.runtime.jit_session] \u001b[94mINFO\u001b[0m: Found JIT Device at COM6\n",
      "08:55:19 [p4jit.p4jit] \u001b[94mINFO\u001b[0m: P4JIT Initialized.\n",
      "\n",
      "Device Memory (Initial):\n",
      "08:55:19 [p4jit.p4jit] \u001b[94mINFO\u001b[0m: [Heap Params]\n",
      "08:55:19 [p4jit.p4jit] \u001b[94mINFO\u001b[0m:   free_spiram    :   31388992 bytes (30653.31 KB)\n",
      "08:55:19 [p4jit.p4jit] \u001b[94mINFO\u001b[0m:   total_spiram   :   33554432 bytes (32768.00 KB)\n",
      "08:55:19 [p4jit.p4jit] \u001b[94mINFO\u001b[0m:   free_internal  :     384063 bytes (375.06 KB)\n",
      "08:55:19 [p4jit.p4jit] \u001b[94mINFO\u001b[0m:   total_internal :     464119 bytes (453.24 KB)\n",
      "\n",
      "================================================================================\n",
      "COMPILING C KERNEL\n",
      "================================================================================\n",
      "08:55:19 [p4jit.p4jit] \u001b[94mINFO\u001b[0m: Loading 'mnist_inference' from 'mnist_inference.c'...\n",
      "08:55:19 [p4jit.toolchain.wrapper_builder] \u001b[94mINFO\u001b[0m: Generating wrapper for 'mnist_inference'\n",
      "08:55:19 [p4jit.toolchain.wrapper_builder] \u001b[94mINFO\u001b[0m: Building wrapper binary...\n",
      "08:55:19 [p4jit.toolchain.builder] \u001b[94mINFO\u001b[0m: Discovered 2 source file(s) in c:\\Users\\orani\\bilel\\git_projects\\robert_manzke\\project1\\trys\\costume_p4code_binary\\P4-JIT\\notebooks\\tutorials\\t02_mnist_classification\\source\n",
      "08:55:20 [p4jit.toolchain.builder] \u001b[94mINFO\u001b[0m: Build validation passed\n",
      "08:55:20 [p4jit.toolchain.wrapper_builder] \u001b[94mINFO\u001b[0m: Wrapper build complete. Metadata saved to c:\\Users\\orani\\bilel\\git_projects\\robert_manzke\\project1\\trys\\costume_p4code_binary\\P4-JIT\\notebooks\\tutorials\\t02_mnist_classification\\build\\signature.json\n",
      "08:55:20 [p4jit.p4jit] \u001b[94mINFO\u001b[0m:   Code Allocated: 0x48210AE0 (4408 bytes)\n",
      "08:55:20 [p4jit.p4jit] \u001b[94mINFO\u001b[0m:   Args Allocated: 0x48211C30 (128 bytes)\n",
      "08:55:20 [p4jit.toolchain.wrapper_builder] \u001b[94mINFO\u001b[0m: Generating wrapper for 'mnist_inference'\n",
      "08:55:20 [p4jit.toolchain.wrapper_builder] \u001b[94mINFO\u001b[0m: Building wrapper binary...\n",
      "08:55:20 [p4jit.toolchain.builder] \u001b[94mINFO\u001b[0m: Discovered 2 source file(s) in c:\\Users\\orani\\bilel\\git_projects\\robert_manzke\\project1\\trys\\costume_p4code_binary\\P4-JIT\\notebooks\\tutorials\\t02_mnist_classification\\source\n",
      "08:55:20 [p4jit.toolchain.builder] \u001b[94mINFO\u001b[0m: Build validation passed\n",
      "08:55:20 [p4jit.toolchain.wrapper_builder] \u001b[94mINFO\u001b[0m: Wrapper build complete. Metadata saved to c:\\Users\\orani\\bilel\\git_projects\\robert_manzke\\project1\\trys\\costume_p4code_binary\\P4-JIT\\notebooks\\tutorials\\t02_mnist_classification\\build\\signature.json\n",
      "08:55:20 [p4jit.p4jit] \u001b[94mINFO\u001b[0m: Function loaded successfully.\n",
      "\n",
      "âœ“ Kernel loaded at: 0x48210AE0\n",
      "âœ“ Binary size: 4344 bytes (4.24 KB)\n",
      "âœ“ Args buffer: 0x48211C30\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ESP32-P4 DEPLOYMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize P4-JIT\n",
    "jit = P4JIT()\n",
    "\n",
    "# Device memory before\n",
    "print(\"\\nDevice Memory (Initial):\")\n",
    "stats_initial = jit.get_heap_stats(print_s=True)\n",
    "\n",
    "# Compile and load inference kernel\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPILING C KERNEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "p4jit.set_log_level('INFO_VERBOSE')\n",
    "\n",
    "func = jit.load(\n",
    "    source=str(c_path),\n",
    "    function_name='mnist_inference',\n",
    "    optimization='O3',\n",
    "    use_firmware_elf=True,\n",
    "    smart_args=False\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Kernel loaded at: 0x{func.code_addr:08X}\")\n",
    "print(f\"âœ“ Binary size: {func.stats['code_size']} bytes ({func.stats['code_size']/1024:.2f} KB)\")\n",
    "print(f\"âœ“ Args buffer: 0x{func.args_addr:08X}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Binary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BINARY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "ELF Sections:\n",
      "08:56:00 [p4jit.toolchain.binary_object] \u001b[94mINFO\u001b[0m: Sections:\n",
      "08:56:00 [p4jit.toolchain.binary_object] \u001b[94mINFO\u001b[0m:   .text                0x48210ae0    4202 bytes\n",
      "08:56:00 [p4jit.toolchain.binary_object] \u001b[94mINFO\u001b[0m:   .rodata              0x48211b4c     139 bytes\n",
      "\n",
      "Symbol Table (Functions):\n",
      "08:56:00 [p4jit.toolchain.binary_object] \u001b[94mINFO\u001b[0m: Functions:\n",
      "08:56:00 [p4jit.toolchain.binary_object] \u001b[94mINFO\u001b[0m:   call_remote                    0x48210ae0  4202 bytes\n",
      "\n",
      "Memory Layout:\n",
      "08:56:00 [p4jit.toolchain.binary_object] \u001b[94mINFO\u001b[0m: Memory Map (Base: 0x48210ae0):\n",
      "08:56:00 [p4jit.toolchain.binary_object] \u001b[94mINFO\u001b[0m:   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "08:56:00 [p4jit.toolchain.binary_object] \u001b[94mINFO\u001b[0m:        0  â”‚ .text          4202 bytes\n",
      "08:56:00 [p4jit.toolchain.binary_object] \u001b[94mINFO\u001b[0m:     4202  â”‚ [padding]         2 bytes\n",
      "08:56:00 [p4jit.toolchain.binary_object] \u001b[94mINFO\u001b[0m:     4204  â”‚ .rodata         139 bytes\n",
      "08:56:00 [p4jit.toolchain.binary_object] \u001b[94mINFO\u001b[0m:     4343  â”‚ [padding]         1 bytes\n",
      "08:56:00 [p4jit.toolchain.binary_object] \u001b[94mINFO\u001b[0m:   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "08:56:00 [p4jit.toolchain.binary_object] \u001b[94mINFO\u001b[0m:   Total: 4344 bytes\n",
      "08:56:00 [p4jit.toolchain.binary_object] \u001b[94mINFO\u001b[0m: Disassembly saved to c:\\Users\\orani\\bilel\\git_projects\\robert_manzke\\project1\\trys\\costume_p4code_binary\\P4-JIT\\notebooks\\tutorials\\t02_mnist_classification\\results\\inference_disasm.txt\n",
      "\n",
      "âœ“ Full disassembly saved to: c:\\Users\\orani\\bilel\\git_projects\\robert_manzke\\project1\\trys\\costume_p4code_binary\\P4-JIT\\notebooks\\tutorials\\t02_mnist_classification\\results\\inference_disasm.txt\n",
      "\n",
      "Disassembly Preview (first 50 lines):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "C:\\Users\\orani\\AppData\\Local\\Temp\\esp32_build_7q0q7wsf\\output.elf:     file format elf32-littleriscv\n",
      "\n",
      "\n",
      "Disassembly of section .text:\n",
      "\n",
      "48210ae0 <call_remote>:\n",
      "48210ae0:\t7119                \taddi\tsp,sp,-128\n",
      "48210ae2:\t48212eb7          \tlui\tt4,0x48212\n",
      "48210ae6:\tdca2                \tsw\ts0,120(sp)\n",
      "48210ae8:\td8ca                \tsw\ts2,112(sp)\n",
      "48210aea:\tc30ea403          \tlw\ts0,-976(t4) # 48211c30 <__binary_end+0x58>\n",
      "48210aee:\td6ce                \tsw\ts3,108(sp)\n",
      "48210af0:\td4d2                \tsw\ts4,104(sp)\n",
      "48210af2:\tcede                \tsw\ts7,92(sp)\n",
      "48210af4:\tc34eaa03          \tlw\ts4,-972(t4)\n",
      "48210af8:\tcce2                \tsw\ts8,88(sp)\n",
      "48210afa:\tc38eab83          \tlw\ts7,-968(t4)\n",
      "48210afe:\tc6ee                \tsw\ts11,76(sp)\n",
      "48210b00:\tc3cea903          \tlw\ts2,-964(t4)\n",
      "48210b04:\tc40ea983          \tlw\ts3,-960(t4)\n",
      "48210b08:\tc44ead83          \tlw\ts11,-956(t4)\n",
      "48210b0c:\tc48eac03          \tlw\ts8,-952(t4)\n",
      "48210b10:\tc4cea683          \tlw\ta3,-948(t4)\n",
      "48210b14:\t48212537          \tlui\ta0,0x48212\n",
      "48210b18:\tb4c50513          \taddi\ta0,a0,-1204 # 48211b4c <call_remote+0x106c>\n",
      "48210b1c:\tde86                \tsw\tra,124(sp)\n",
      "48210b1e:\tdaa6                \tsw\ts1,116(sp)\n",
      "48210b20:\td2d6                \tsw\ts5,100(sp)\n",
      "48210b22:\td0da                \tsw\ts6,96(sp)\n",
      "48210b24:\tcae6                \tsw\ts9,84(sp)\n",
      "48210b26:\tc50eab03          \tlw\ts6,-944(t4)\n",
      "48210b2a:\td636                \tsw\ta3,44(sp)\n",
      "48210b2c:\tc54ea483          \tlw\ts1,-940(t4)\n",
      "48210b30:\tc8ea                \tsw\ts10,80(sp)\n",
      "48210b32:\tf7e06097          \tauipc\tra,0xf7e06\n",
      "48210b36:\t32a080e7          \tjalr\t810(ra) # 40016e5c <printf>\n",
      "48210b3a:\t48212537          \tlui\ta0,0x48212\n",
      "48210b3e:\tb7050513          \taddi\ta0,a0,-1168 # 48211b70 <call_remote+0x1090>\n",
      "48210b42:\tf7e06097          \tauipc\tra,0xf7e06\n",
      "48210b46:\t31a080e7          \tjalr\t794(ra) # 40016e5c <printf>\n",
      "48210b4a:\t001a0793          \taddi\ta5,s4,1\n",
      "48210b4e:\td03e                \tsw\ta5,32(sp)\n",
      "48210b50:\t002a0793          \taddi\ta5,s4,2\n",
      "48210b54:\tce3e                \tsw\ta5,28(sp)\n",
      "48210b56:\t004a0793          \taddi\ta5,s4,4\n",
      "48210b5a:\tc83e                \tsw\ta5,16(sp)\n",
      "48210b5c:\t007a0793          \taddi\ta5,s4,7\n",
      "48210b60:\tca3e                \tsw\ta5,20(sp)\n",
      "48210b62:\t006a0793          \taddi\ta5,s4,6\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BINARY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Sections\n",
    "print(\"\\nELF Sections:\")\n",
    "func.binary.print_sections()\n",
    "\n",
    "# Symbol table\n",
    "print(\"\\nSymbol Table (Functions):\")\n",
    "func.binary.print_symbols()\n",
    "\n",
    "# Memory layout\n",
    "print(\"\\nMemory Layout:\")\n",
    "func.binary.print_memory_map()\n",
    "\n",
    "# Disassembly\n",
    "disasm_path = RESULTS_DIR / 'inference_disasm.txt'\n",
    "func.binary.disassemble(output=str(disasm_path), source_intermix=False)\n",
    "print(f\"\\nâœ“ Full disassembly saved to: {disasm_path}\")\n",
    "\n",
    "# Show first 50 lines of disassembly\n",
    "print(\"\\nDisassembly Preview (first 50 lines):\")\n",
    "print(\"-\" * 80)\n",
    "with open(disasm_path, 'r') as f:\n",
    "    lines = f.readlines()[:50]\n",
    "    print(''.join(lines))\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Weights to Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "WEIGHT UPLOAD\n",
      "================================================================================\n",
      "CONV1  | W: 0x48211CD0 (   144 bytes) | B: 0x48211D80 (  16 bytes)\n",
      "CONV2  | W: 0x48211DB0 (  4608 bytes) | B: 0x48212FD0 (  32 bytes)\n",
      "FC1    | W: 0x48213010 (200704 bytes) | B: 0x48244030 ( 128 bytes)\n",
      "FC2    | W: 0x482440D0 (  1280 bytes) | B: 0x482445F0 (  10 bytes)\n",
      "\n",
      "Scratch buffer: 0x48244600 (65536 bytes)\n",
      "\n",
      "Device Memory (After Upload):\n",
      "08:56:43 [p4jit.p4jit] \u001b[94mINFO\u001b[0m: [Heap Params]\n",
      "08:56:43 [p4jit.p4jit] \u001b[94mINFO\u001b[0m:   free_spiram    :   31111952 bytes (30382.77 KB)\n",
      "08:56:43 [p4jit.p4jit] \u001b[94mINFO\u001b[0m:   total_spiram   :   33554432 bytes (32768.00 KB)\n",
      "08:56:43 [p4jit.p4jit] \u001b[94mINFO\u001b[0m:   free_internal  :     384063 bytes (375.06 KB)\n",
      "08:56:43 [p4jit.p4jit] \u001b[94mINFO\u001b[0m:   total_internal :     464119 bytes (453.24 KB)\n",
      "\n",
      "âœ“ Total memory used: 270.55 KB\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "device = jit.session.device\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WEIGHT UPLOAD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Allocate and upload each layer's weights\n",
    "weight_addrs = {}\n",
    "\n",
    "for layer_name in ['conv1', 'conv2', 'fc1', 'fc2']:\n",
    "    layer = quantized_weights[layer_name]\n",
    "    \n",
    "    # Weights\n",
    "    w_bytes = layer['weight'].tobytes()\n",
    "    w_addr = device.allocate(len(w_bytes), MALLOC_CAP_SPIRAM | MALLOC_CAP_8BIT, 16)\n",
    "    device.write_memory(w_addr, w_bytes)\n",
    "    \n",
    "    # Biases\n",
    "    b_bytes = layer['bias'].tobytes()\n",
    "    b_addr = device.allocate(len(b_bytes), MALLOC_CAP_SPIRAM | MALLOC_CAP_8BIT, 16)\n",
    "    device.write_memory(b_addr, b_bytes)\n",
    "    \n",
    "    weight_addrs[f'{layer_name}_w'] = w_addr\n",
    "    weight_addrs[f'{layer_name}_b'] = b_addr\n",
    "    \n",
    "    print(f\"{layer_name.upper():6s} | W: 0x{w_addr:08X} ({len(w_bytes):6d} bytes) | \"\n",
    "          f\"B: 0x{b_addr:08X} ({len(b_bytes):4d} bytes)\")\n",
    "\n",
    "# Allocate scratch buffer\n",
    "scratch_size = 64 * 1024  # 64 KB\n",
    "scratch_addr = device.allocate(scratch_size, MALLOC_CAP_SPIRAM | MALLOC_CAP_8BIT, 16)\n",
    "print(f\"\\nScratch buffer: 0x{scratch_addr:08X} ({scratch_size} bytes)\")\n",
    "\n",
    "# Memory after upload\n",
    "print(\"\\nDevice Memory (After Upload):\")\n",
    "stats_uploaded = jit.get_heap_stats(print_s=True)\n",
    "\n",
    "memory_used = (stats_initial['free_spiram'] - stats_uploaded['free_spiram']) / 1024\n",
    "print(f\"\\nâœ“ Total memory used: {memory_used:.2f} KB\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Inference on Hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INFERENCE ON ESP32-P4 @ 360 MHz\n",
      "================================================================================\n",
      "\n",
      "Testing digit 0...\n",
      "  âœ— True: 0 | Predicted: 9 | Time: 55.93 ms\n",
      "\n",
      "Testing digit 1...\n",
      "  âœ— True: 1 | Predicted: 0 | Time: 55.41 ms\n",
      "\n",
      "Testing digit 2...\n",
      "  âœ— True: 2 | Predicted: 0 | Time: 56.70 ms\n",
      "\n",
      "Testing digit 3...\n",
      "  âœ— True: 3 | Predicted: 9 | Time: 56.22 ms\n",
      "\n",
      "Testing digit 4...\n",
      "  âœ— True: 4 | Predicted: 9 | Time: 56.47 ms\n",
      "\n",
      "Testing digit 5...\n",
      "  âœ— True: 5 | Predicted: 9 | Time: 56.52 ms\n",
      "\n",
      "Testing digit 6...\n",
      "  âœ— True: 6 | Predicted: 0 | Time: 55.60 ms\n",
      "\n",
      "Testing digit 7...\n",
      "  âœ— True: 7 | Predicted: 0 | Time: 55.34 ms\n",
      "\n",
      "Testing digit 8...\n",
      "  âœ“ True: 8 | Predicted: 8 | Time: 55.40 ms\n",
      "\n",
      "Testing digit 9...\n",
      "  âœ— True: 9 | Predicted: 8 | Time: 56.44 ms\n",
      "\n",
      "================================================================================\n",
      "RESULTS SUMMARY\n",
      "================================================================================\n",
      "âœ“ Accuracy: 1/10 = 10.0%\n",
      "âœ“ Avg inference time: 56.00 ms\n",
      "âœ“ Min/Max time: 55.34 / 56.70 ms\n",
      "âœ“ Throughput: 17.9 inferences/second\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INFERENCE ON ESP32-P4 @ 360 MHz\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import time\n",
    "results = {}\n",
    "\n",
    "for digit in range(10):\n",
    "    print(f\"\\nTesting digit {digit}...\")\n",
    "    \n",
    "    # Upload input image\n",
    "    img_bytes = test_images_int8[digit].tobytes()\n",
    "    input_addr = device.allocate(len(img_bytes), MALLOC_CAP_SPIRAM | MALLOC_CAP_8BIT, 16)\n",
    "    device.write_memory(input_addr, img_bytes)\n",
    "    \n",
    "    # Prepare function arguments\n",
    "    # mnist_inference(input, w1, b1, w2, b2, w_fc1, b_fc1, w_fc2, b_fc2, scratch)\n",
    "    args_blob = struct.pack('<IIIIIIIIII',\n",
    "        input_addr,\n",
    "        weight_addrs['conv1_w'], weight_addrs['conv1_b'],\n",
    "        weight_addrs['conv2_w'], weight_addrs['conv2_b'],\n",
    "        weight_addrs['fc1_w'], weight_addrs['fc1_b'],\n",
    "        weight_addrs['fc2_w'], weight_addrs['fc2_b'],\n",
    "        scratch_addr\n",
    "    )\n",
    "    \n",
    "    # Execute\n",
    "    start = time.time()\n",
    "    func(args_blob)\n",
    "    duration_ms = (time.time() - start) * 1000\n",
    "    \n",
    "    # Read result (slot 31)\n",
    "    result_addr = func.args_addr + 124\n",
    "    result_bytes = device.read_memory(result_addr, 4)\n",
    "    predicted = struct.unpack('<i', result_bytes)[0]\n",
    "    \n",
    "    # Cleanup input\n",
    "    device.free(input_addr)\n",
    "    \n",
    "    # Store results\n",
    "    results[digit] = {\n",
    "        'true': digit,\n",
    "        'predicted': predicted,\n",
    "        'correct': (predicted == digit),\n",
    "        'time_ms': duration_ms\n",
    "    }\n",
    "    \n",
    "    status = \"âœ“\" if predicted == digit else \"âœ—\"\n",
    "    print(f\"  {status} True: {digit} | Predicted: {predicted} | Time: {duration_ms:.2f} ms\")\n",
    "\n",
    "# Summary\n",
    "correct = sum(1 for r in results.values() if r['correct'])\n",
    "accuracy = 100.0 * correct / len(results)\n",
    "avg_time = np.mean([r['time_ms'] for r in results.values()])\n",
    "min_time = np.min([r['time_ms'] for r in results.values()])\n",
    "max_time = np.max([r['time_ms'] for r in results.values()])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"âœ“ Accuracy: {correct}/{len(results)} = {accuracy:.1f}%\")\n",
    "print(f\"âœ“ Avg inference time: {avg_time:.2f} ms\")\n",
    "print(f\"âœ“ Min/Max time: {min_time:.2f} / {max_time:.2f} ms\")\n",
    "print(f\"âœ“ Throughput: {1000/avg_time:.1f} inferences/second\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion = np.zeros((10, 10), dtype=int)\n",
    "for r in results.values():\n",
    "    confusion[r['true'], r['predicted']] += 1\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Confusion matrix\n",
    "im = ax1.imshow(confusion, cmap='Blues')\n",
    "ax1.set_xticks(np.arange(10))\n",
    "ax1.set_yticks(np.arange(10))\n",
    "ax1.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "ax1.set_title(f'Confusion Matrix\\nAccuracy: {accuracy:.1f}%', \n",
    "              fontsize=14, fontweight='bold')\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        text = ax1.text(j, i, confusion[i, j],\n",
    "                       ha=\"center\", va=\"center\",\n",
    "                       color=\"white\" if confusion[i, j] > 0.5 else \"black\",\n",
    "                       fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax1)\n",
    "\n",
    "# Inference time\n",
    "times = [results[d]['time_ms'] for d in range(10)]\n",
    "colors = ['green' if results[d]['correct'] else 'red' for d in range(10)]\n",
    "\n",
    "bars = ax2.bar(range(10), times, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax2.axhline(avg_time, color='blue', linestyle='--', linewidth=2, \n",
    "            label=f'Average: {avg_time:.2f} ms')\n",
    "ax2.set_xlabel('Digit Class', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Inference Time (ms)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Inference Time per Class', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(range(10))\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'inference_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory breakdown\n",
    "memory_breakdown = {\n",
    "    'Code': func.stats['code_size'] / 1024,\n",
    "    'Conv1 Weights': (quantized_weights['conv1']['weight'].size + \n",
    "                      quantized_weights['conv1']['bias'].size) / 1024,\n",
    "    'Conv2 Weights': (quantized_weights['conv2']['weight'].size + \n",
    "                      quantized_weights['conv2']['bias'].size) / 1024,\n",
    "    'FC1 Weights': (quantized_weights['fc1']['weight'].size + \n",
    "                    quantized_weights['fc1']['bias'].size) / 1024,\n",
    "    'FC2 Weights': (quantized_weights['fc2']['weight'].size + \n",
    "                    quantized_weights['fc2']['bias'].size) / 1024,\n",
    "    'Scratch Buffer': scratch_size / 1024\n",
    "}\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Pie chart\n",
    "colors_pie = plt.cm.Set3(np.linspace(0, 1, len(memory_breakdown)))\n",
    "wedges, texts, autotexts = ax1.pie(memory_breakdown.values(), \n",
    "                                     labels=memory_breakdown.keys(),\n",
    "                                     autopct='%1.1f%%',\n",
    "                                     colors=colors_pie,\n",
    "                                     startangle=90)\n",
    "\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('black')\n",
    "    autotext.set_fontweight('bold')\n",
    "    autotext.set_fontsize(10)\n",
    "\n",
    "ax1.set_title(f'Memory Usage Breakdown\\nTotal: {memory_used:.2f} KB', \n",
    "              fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "bars = ax2.barh(list(memory_breakdown.keys()), list(memory_breakdown.values()),\n",
    "                color=colors_pie, edgecolor='black', linewidth=1.5)\n",
    "ax2.set_xlabel('Size (KB)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Memory Components', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add values on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    ax2.text(width, bar.get_y() + bar.get_height()/2,\n",
    "            f'{width:.2f} KB',\n",
    "            ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'memory_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Final Report & Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SYSTEM SPECIFICATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“Š Model Architecture:\")\n",
    "print(f\"  â€¢ Input: 28Ã—28 grayscale (784 pixels)\")\n",
    "print(f\"  â€¢ Conv1: 1â†’16 channels, 3Ã—3 kernel, ReLU, MaxPool\")\n",
    "print(f\"  â€¢ Conv2: 16â†’32 channels, 3Ã—3 kernel, ReLU, MaxPool\")\n",
    "print(f\"  â€¢ FC1: 800â†’128, ReLU\")\n",
    "print(f\"  â€¢ FC2: 128â†’10 (output logits)\")\n",
    "print(f\"  â€¢ Total parameters: {total_int8_params:,} (INT8)\")\n",
    "\n",
    "print(\"\\nâš¡ Hardware Performance:\")\n",
    "print(f\"  â€¢ Platform: ESP32-P4 @ 360 MHz\")\n",
    "print(f\"  â€¢ Architecture: RISC-V RV32IMAFC\")\n",
    "print(f\"  â€¢ Memory: {stats_initial['total_spiram']//1024//1024} MB SPIRAM\")\n",
    "print(f\"  â€¢ Code size: {func.stats['code_size']/1024:.2f} KB\")\n",
    "print(f\"  â€¢ Avg inference: {avg_time:.2f} ms\")\n",
    "print(f\"  â€¢ Throughput: {1000/avg_time:.1f} fps\")\n",
    "print(f\"  â€¢ Accuracy: {accuracy:.1f}%\")\n",
    "\n",
    "print(\"\\nðŸ’¾ Memory Footprint:\")\n",
    "print(f\"  â€¢ Weights (INT8): {total_int8_params/1024:.2f} KB\")\n",
    "print(f\"  â€¢ Code: {func.stats['code_size']/1024:.2f} KB\")\n",
    "print(f\"  â€¢ Scratch buffer: {scratch_size/1024:.2f} KB\")\n",
    "print(f\"  â€¢ Total SPIRAM: {memory_used:.2f} KB\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Quantization:\")\n",
    "print(f\"  â€¢ Weight precision: INT8 (8-bit)\")\n",
    "print(f\"  â€¢ Activation precision: INT8 (8-bit)\")\n",
    "print(f\"  â€¢ Accumulator: INT32 (32-bit)\")\n",
    "print(f\"  â€¢ Scaling: Power-of-2 (bit-shift)\")\n",
    "print(f\"  â€¢ Compression: 4Ã— vs FP32\")\n",
    "print(f\"  â€¢ Training method: QAT with STE\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Comparison:\")\n",
    "print(f\"  â€¢ FP32 model size: {total_fp32_params/1024:.2f} KB\")\n",
    "print(f\"  â€¢ INT8 model size: {total_int8_params/1024:.2f} KB\")\n",
    "print(f\"  â€¢ Size reduction: {(1-total_int8_params/total_fp32_params)*100:.1f}%\")\n",
    "print(f\"  â€¢ Training accuracy: {history['test_acc'][-1]:.2f}%\")\n",
    "print(f\"  â€¢ On-device accuracy: {accuracy:.1f}%\")\n",
    "print(f\"  â€¢ Accuracy retention: {accuracy/history['test_acc'][-1]*100:.1f}%\")\n",
    "\n",
    "print(\"\\nðŸš€ P4-JIT Advantages:\")\n",
    "print(\"  1. Deploy time: 2-3 seconds (vs 30-60s firmware rebuild)\")\n",
    "print(\"  2. No firmware changes required\")\n",
    "print(\"  3. Native RISC-V execution (zero interpreter overhead)\")\n",
    "print(\"  4. Dynamic code loading via USB\")\n",
    "print(\"  5. Seamless Python â†’ C â†’ Hardware workflow\")\n",
    "print(\"  6. Real-time performance monitoring\")\n",
    "print(\"  7. Comprehensive binary introspection\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ¨ DEMONSTRATION COMPLETE âœ¨\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLEANUP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Free weights\n",
    "for addr in weight_addrs.values():\n",
    "    device.free(addr)\n",
    "\n",
    "# Free scratch\n",
    "device.free(scratch_addr)\n",
    "\n",
    "# Free function\n",
    "func.free()\n",
    "\n",
    "print(\"âœ“ All device memory freed\")\n",
    "\n",
    "# Final stats\n",
    "stats_final = jit.get_heap_stats(print_s=False)\n",
    "reclaimed = (stats_final['free_spiram'] - stats_uploaded['free_spiram']) / 1024\n",
    "\n",
    "print(f\"âœ“ Memory reclaimed: {reclaimed:.2f} KB\")\n",
    "print(f\"âœ“ Final free SPIRAM: {stats_final['free_spiram']//1024//1024} MB\")\n",
    "\n",
    "# Disconnect\n",
    "jit.session.device.disconnect()\n",
    "print(\"âœ“ Device disconnected\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"All results saved to: {RESULTS_DIR}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
